{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehdtjr1220/miniconda3/envs/proj2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json \n",
    "from math_utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from parser import *  \n",
    "from grader import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')  \n",
    "\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_path = f\"../data/math500/test.jsonl\"\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test = [json.loads(line) for line in f]\n",
    "\n",
    "for _type in [\"zero\", \"few\"]:\n",
    "    for model in ['gpt-4o-mini', 'gpt-4o']:\n",
    "        file_path = f\"../result/math500/{model}/math500_{_type}.jsonl\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "\n",
    "        scores = [[] for _ in range(5)]\n",
    "\n",
    "        any_correct = []\n",
    "\n",
    "        all_correct_count = 0\n",
    "\n",
    "        for entry in data:\n",
    "            idx = entry[\"idx\"]\n",
    "\n",
    "            preds = [extract_answer(mo, \"math\") for mo in entry['model_outputs']]\n",
    "\n",
    "            _, gt = parse_ground_truth(test[idx], \"math\")  \n",
    "\n",
    "            sample_results = []  \n",
    "\n",
    "            for i, pred in enumerate(preds):\n",
    "                result = math_equal_process((None, pred, gt))\n",
    "                if not result: \n",
    "                    result = process_results(gt, [entry['model_outputs'][i]])\n",
    "                    if not result:\n",
    "                        pred = extract_answer(pred, \"math\")\n",
    "                        result = math_equal_process((None, pred, gt))\n",
    "                sample_results.append(result)\n",
    "                scores[i].append(result)\n",
    "\n",
    "\n",
    "            any_correct.append(any(sample_results))\n",
    "            if all(sample_results):\n",
    "                all_correct_count += 1\n",
    "\n",
    "        num_questions = len(data)\n",
    "\n",
    "        worse_case_acc = all_correct_count / num_questions if num_questions > 0 else 0.0\n",
    "\n",
    "        rep_accuracies = []\n",
    "        for i in range(5):\n",
    "            rep_acc = sum(scores[i]) / num_questions if num_questions > 0 else 0.0\n",
    "            rep_accuracies.append(rep_acc)\n",
    "\n",
    "        avg_rep_acc = sum(rep_accuracies) / 5.0\n",
    "\n",
    "\n",
    "        any_correct_acc = sum(any_correct) / num_questions if num_questions > 0 else 0.0\n",
    "\n",
    "        print(f\"Results for {_type} using model {model}:\")\n",
    "        print(f\"  Worse-case (all correct in a single question): {worse_case_acc:.3f}\")\n",
    "\n",
    "        for i, acc_val in enumerate(rep_accuracies):\n",
    "            print(f\"  Repetition {i}: {acc_val:.3f}\")\n",
    "\n",
    "        print(f\"  Average (across 5 samples): {avg_rep_acc:.3f}\")\n",
    "        print(f\"  Any-correct Accuracy: {any_correct_acc:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  0.416\n",
      "1:  0.442\n",
      "2:  0.458\n",
      "3:  0.42\n",
      "4:  0.41\n",
      "few Avg:  0.42919999999999997\n",
      "Any_correct:  0.672\n",
      "0:  0.412\n",
      "1:  0.446\n",
      "2:  0.442\n",
      "3:  0.45\n",
      "4:  0.458\n",
      "zero Avg:  0.44160000000000005\n",
      "Any_correct:  0.69\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "llama-3.1-8B-Instruct\n",
    "few,zero CoT\n",
    "do_sample=True, temperatue=1, repeats=5\n",
    "'''\n",
    "test_path = f\"../data/math500/test.jsonl\"\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test = [json.loads(line) for line in f]\n",
    "\n",
    "for _type in ['few', 'zero']:\n",
    "\n",
    "    file_path = f\"../result/math500/llama/math500_{_type}.jsonl\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    n = len(data[0]['resps'][0])\n",
    "    any_correct = 0 \n",
    "    idx_acc = [[] for i in range(n)]\n",
    "    for idx, entry in enumerate(data):\n",
    "        flag = False\n",
    "\n",
    "        _, gt = parse_ground_truth(test[idx], \"math\")  \n",
    "        \n",
    "        for idx, resps in enumerate(entry['resps'][0]):\n",
    "            pred = extract_answer(resps, \"math\")\n",
    "            pred = strip_string(pred)\n",
    "            \n",
    "            result = math_equal_process((idx, pred, gt))\n",
    "            if not result:\n",
    "                result = process_results(gt, [resps])\n",
    "                if not result:\n",
    "                    pred = extract_answer(pred, \"math\")\n",
    "                    result = math_equal_process((None, pred, gt))\n",
    "            idx_acc[idx].append(int(result))\n",
    "            \n",
    "            if result: \n",
    "                flag = True\n",
    "        if flag:\n",
    "            any_correct += 1\n",
    "    total = 0 \n",
    "    for idx, acc in enumerate(idx_acc): \n",
    "        print(f\"{idx}: \", sum(acc)/500)\n",
    "        total += sum(acc) / 500\n",
    "    print(f\"{_type} Avg: \", total / n)\n",
    "    print(\"Any_correct: \", any_correct/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_predictions_with_is_correct(file_path, data_path):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: JSON file not found at {file_path}\")\n",
    "        return\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = [cand_list[:500] for cand_list in json_data]\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: jsonl file not found at {data_path}\")\n",
    "        return\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    problem_groups = list(zip(*json_data))[:500]\n",
    "    cnt = 0\n",
    "    for problem_likelihoods in tqdm(problem_groups, desc=\"Updating problems\"):\n",
    "        problem_list = list(problem_likelihoods)\n",
    "        idx = problem_list[0].get(\"id\") \n",
    "    \n",
    "        _, gt = parse_ground_truth(data[cnt], \"math\")\n",
    "\n",
    "        for cl in problem_list: \n",
    "            if cl['id'] != cnt: \n",
    "                cl['id'] = cnt \n",
    "            pred = extract_answer(cl['model_output'], \"math\")\n",
    "            pred = strip_string(pred)\n",
    "            cl['pred'] = pred\n",
    "            \n",
    "            result = math_equal_process((None, pred, gt)) \n",
    "            if not result: \n",
    "                try:\n",
    "                    result = process_results(gt, [cl['model_output']])\n",
    "                    if not result:\n",
    "                        pred = extract_answer(pred, \"math\")\n",
    "                        result = math_equal_process((None, pred, gt))\n",
    "                except AssertionError:\n",
    "                    print(cl['model_output'])\n",
    "                    result = 0\n",
    "            \n",
    "            cl['is_correct'] = result\n",
    "        cnt += 1\n",
    "    \n",
    "    print(len(json_data))\n",
    "    print(len(json_data[0]))\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Updated predictions saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating problems: 100%|██████████| 500/500 [00:33<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "500\n",
      "Updated predictions saved at: ../likelihood_diff/math500/gpt-4o-mini/few/all_likelihoods_2025.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"../likelihood/math500\"\n",
    "\n",
    "models = ['gpt-4o-mini']\n",
    "file_name = 'few/all_likelihoods.json'\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"{output_dir}/{model}/{file_name}\"\n",
    "    data_path = f\"../data/math500/test.jsonl\"\n",
    "    \n",
    "    update_predictions_with_is_correct(file_path, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
