{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehdtjr1220/miniconda3/envs/proj2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Iterable, Union, Any\n",
    "from pathlib import Path\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from mmlu_utils import * \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')  \n",
    "\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigned subjects ['business', 'law', 'psychology', 'biology', 'chemistry', 'history', 'other', 'health', 'economics', 'math', 'physics', 'computer science', 'philosophy', 'engineering']\n"
     ]
    }
   ],
   "source": [
    "def load_mmlu_pro():\n",
    "    dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
    "    test_df, val_df = dataset[\"test\"], dataset[\"validation\"]\n",
    "    test_df = preprocess(test_df)\n",
    "    val_df = preprocess(val_df)\n",
    "    return test_df, val_df\n",
    "\n",
    "\n",
    "def preprocess(test_df):\n",
    "    res_df = []\n",
    "    for each in test_df:\n",
    "        options = [opt for opt in each[\"options\"] if opt != \"N/A\"]\n",
    "        each[\"options\"] = options\n",
    "        res_df.append(each)\n",
    "    res = {}\n",
    "    for each in res_df:\n",
    "        if each[\"category\"] not in res:\n",
    "            res[each[\"category\"]] = []\n",
    "        res[each[\"category\"]].append(each)\n",
    "    return res\n",
    "\n",
    "def load_jsonl(file: Union[str, Path]) -> Iterable[Any]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except:\n",
    "                print(\"Error in loading:\", line)\n",
    "                exit()\n",
    "\n",
    "test_df, dev_df = load_mmlu_pro()\n",
    "subjects = list(test_df.keys())\n",
    "\n",
    "print(\"assigned subjects\", subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../result/mmlu_pro/llama/mmlu_pro_few.jsonl\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    llama_data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "grouped_entries = defaultdict(list)\n",
    "for entry in llama_data:\n",
    "    subject = entry['doc']['category'] # entry['entry']['category'] \n",
    "    grouped_entries[subject].append(entry)\n",
    "\n",
    "\n",
    "output_dir = \"../result/mmlu_pro\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for subject, entries in grouped_entries.items():\n",
    "    save_path = os.path.join(output_dir, f\"{subject}_result.jsonl\")\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in entries:\n",
    "            line = json.dumps(entry, ensure_ascii=False)\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"Subject: {subject} - Total entries: {len(entries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for model=gpt-4o, type=_zero_result ===\n",
      "Subject                     len       Idx0      Idx1      Idx2      Idx3      Idx4   Self-Consistency   Anycorrect  \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "business                    300      75.33%    76.67%    76.67%    74.00%    76.00%       78.33%         87.67%    \n",
      "law                         300      59.67%    61.67%    59.00%    59.33%    58.00%       61.67%         72.00%    \n",
      "psychology                  300      81.00%    79.00%    78.00%    80.00%    79.33%       79.00%         87.67%    \n",
      "biology                     300      85.00%    87.33%    87.33%    87.33%    88.33%       87.33%         92.33%    \n",
      "chemistry                   300      70.00%    71.00%    71.67%    68.67%    68.00%       74.33%         87.00%    \n",
      "history                     300      69.33%    70.33%    71.00%    71.67%    70.33%       71.33%         77.00%    \n",
      "other                       300      76.00%    75.67%    79.33%    76.33%    75.67%       78.00%         85.00%    \n",
      "health                      300      75.67%    77.00%    76.67%    76.33%    76.67%       77.00%         82.67%    \n",
      "economics                   300      81.00%    80.00%    80.33%    80.00%    80.33%       81.67%         87.33%    \n",
      "math                        300      79.33%    78.33%    80.67%    81.00%    77.67%       83.33%         93.00%    \n",
      "physics                     300      77.00%    75.67%    77.33%    74.00%    77.67%       80.67%         92.00%    \n",
      "computer science            300      77.67%    78.00%    75.67%    74.00%    77.67%       79.00%         87.33%    \n",
      "philosophy                  300      71.67%    69.33%    71.00%    71.33%    70.67%       72.33%         81.00%    \n",
      "engineering                 300      58.00%    52.33%    53.67%    55.67%    56.33%       61.33%         81.33%    \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL (Weighted)                     74.05%    73.74%    74.17%    73.55%    73.76%       76.10%         85.24%    \n",
      "TOTAL (Simple)                       74.05%    73.74%    74.17%    73.55%    73.76%       76.10%         85.24%    \n",
      "Some custom measure: 0.7385\n",
      "\n",
      "=== Results for model=gpt-4o, type=_result ===\n",
      "Subject                     len       Idx0      Idx1      Idx2      Idx3      Idx4   Self-Consistency   Anycorrect  \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "business                    300      74.67%    77.33%    75.67%    72.67%    73.67%       79.00%         88.33%    \n",
      "law                         300      59.00%    57.33%    56.33%    59.00%    54.67%       59.00%         70.00%    \n",
      "psychology                  300      80.00%    79.67%    82.00%    81.00%    80.00%       81.67%         86.67%    \n",
      "biology                     300      88.33%    88.00%    87.67%    89.00%    88.00%       89.67%         92.33%    \n",
      "chemistry                   300      70.67%    69.33%    70.00%    70.67%    68.67%       73.67%         86.33%    \n",
      "history                     300      73.33%    73.67%    73.67%    74.67%    72.00%       75.33%         79.33%    \n",
      "other                       300      76.33%    74.67%    77.67%    76.67%    77.67%       78.00%         84.00%    \n",
      "health                      300      76.00%    76.00%    76.00%    76.00%    76.67%       78.33%         82.33%    \n",
      "economics                   300      81.33%    77.67%    80.33%    77.33%    77.00%       80.67%         86.00%    \n",
      "math                        300      80.33%    79.33%    81.33%    82.33%    81.33%       84.33%         93.33%    \n",
      "physics                     300      79.33%    78.67%    78.00%    77.67%    76.00%       82.67%         90.67%    \n",
      "computer science            300      73.33%    77.33%    75.67%    77.67%    75.00%       79.00%         86.00%    \n",
      "philosophy                  300      71.00%    67.67%    68.67%    69.00%    70.67%       69.33%         81.33%    \n",
      "engineering                 300      52.33%    49.33%    52.00%    52.67%    55.67%       58.33%         79.67%    \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL (Weighted)                     74.00%    73.29%    73.93%    74.02%    73.36%       76.36%         84.74%    \n",
      "TOTAL (Simple)                       74.00%    73.29%    73.93%    74.02%    73.36%       76.36%         84.74%    \n",
      "Some custom measure: 0.7372\n",
      "\n",
      "=== Results for model=gpt-4o-mini, type=_zero_result ===\n",
      "Subject                     len       Idx0      Idx1      Idx2      Idx3      Idx4   Self-Consistency   Anycorrect  \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "business                    300      68.33%    68.33%    71.67%    66.33%    66.33%       72.33%         83.67%    \n",
      "law                         300      34.67%    38.67%    40.33%    37.33%    36.00%       38.67%         58.33%    \n",
      "psychology                  300      72.00%    73.33%    73.00%    73.67%    70.33%       72.33%         82.33%    \n",
      "biology                     300      81.67%    79.67%    79.33%    82.67%    81.00%       82.00%         88.33%    \n",
      "chemistry                   300      61.67%    64.33%    59.67%    64.00%    56.33%       65.00%         81.00%    \n",
      "history                     300      58.00%    57.00%    58.67%    59.33%    56.67%       58.33%         71.33%    \n",
      "other                       300      67.67%    68.33%    69.33%    67.33%    66.67%       68.33%         80.33%    \n",
      "health                      300      70.67%    71.33%    70.33%    69.00%    69.00%       70.67%         79.33%    \n",
      "economics                   300      68.33%    72.33%    71.00%    74.33%    72.67%       73.00%         83.00%    \n",
      "math                        300      64.67%    68.33%    70.00%    67.33%    68.33%       71.33%         86.67%    \n",
      "physics                     300      66.67%    62.67%    64.00%    66.00%    64.67%       70.67%         83.00%    \n",
      "computer science            300      69.00%    66.00%    66.00%    68.33%    66.33%       66.33%         82.33%    \n",
      "philosophy                  300      55.67%    55.67%    57.00%    56.33%    54.00%       57.00%         69.67%    \n",
      "engineering                 300      41.00%    39.33%    34.00%    39.00%    39.00%       42.67%         66.33%    \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL (Weighted)                     62.86%    63.24%    63.17%    63.64%    61.95%       64.90%         78.26%    \n",
      "TOTAL (Simple)                       62.86%    63.24%    63.17%    63.64%    61.95%       64.90%         78.26%    \n",
      "Some custom measure: 0.6297\n",
      "\n",
      "=== Results for model=gpt-4o-mini, type=_result ===\n",
      "Subject                     len       Idx0      Idx1      Idx2      Idx3      Idx4   Self-Consistency   Anycorrect  \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "business                    300      68.67%    62.00%    63.67%    63.00%    69.00%       69.33%         81.00%    \n",
      "law                         300      37.00%    37.00%    37.33%    35.33%    38.00%       39.33%         53.33%    \n",
      "psychology                  300      73.00%    73.33%    73.67%    73.00%    73.00%       75.00%         82.00%    \n",
      "biology                     300      80.00%    80.00%    81.67%    81.00%    81.00%       82.00%         88.00%    \n",
      "chemistry                   300      61.33%    58.33%    62.00%    62.33%    61.33%       67.00%         77.00%    \n",
      "history                     300      56.67%    56.00%    57.67%    57.00%    60.00%       58.00%         68.67%    \n",
      "other                       300      66.67%    65.67%    67.00%    65.33%    67.33%       69.00%         79.00%    \n",
      "health                      300      71.33%    69.00%    67.33%    71.67%    67.67%       71.33%         78.33%    \n",
      "economics                   300      71.67%    72.00%    71.00%    71.67%    73.00%       73.33%         80.67%    \n",
      "math                        300      75.00%    77.67%    74.67%    74.00%    75.67%       77.00%         87.67%    \n",
      "physics                     300      66.33%    65.33%    66.00%    66.00%    64.00%       68.00%         83.67%    \n",
      "computer science            300      69.00%    66.67%    68.33%    69.00%    67.33%       69.67%         82.00%    \n",
      "philosophy                  300      51.33%    55.67%    53.00%    54.33%    53.67%       54.00%         67.00%    \n",
      "engineering                 300      37.67%    38.67%    36.00%    35.00%    39.00%       41.67%         63.00%    \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL (Weighted)                     63.26%    62.67%    62.81%    62.76%    63.57%       65.33%         76.52%    \n",
      "TOTAL (Simple)                       63.26%    62.67%    62.81%    62.76%    63.57%       65.33%         76.52%    \n",
      "Some custom measure: 0.6301\n"
     ]
    }
   ],
   "source": [
    "models = ['gpt-4o', 'gpt-4o-mini']\n",
    "types = ['_zero_result', '_result']\n",
    "\n",
    "for model in models:\n",
    "    for t in types:\n",
    "        zero_result = {}\n",
    "\n",
    "        # --------------------\n",
    "        # 1) Collect results\n",
    "        # --------------------\n",
    "        for subject in subjects:\n",
    "            output_res_path = os.path.join(f\"../result/mmlu_pro/{model}/\", subject + f\"{t}.jsonl\")\n",
    "            if not os.path.exists(output_res_path):\n",
    "                print(f\"[WARNING] File not found: {output_res_path}\")\n",
    "                continue\n",
    "\n",
    "            with open(output_res_path, 'r', encoding='utf-8') as f:\n",
    "                res = [json.loads(line) for line in f]\n",
    "\n",
    "            if not res:\n",
    "                print(f\"[WARNING] No predictions for subject={subject} (model={model}, type={t})\")\n",
    "                continue\n",
    "\n",
    "            # Number of questions\n",
    "            N = len(res)\n",
    "            # Number of predictions per question\n",
    "            num_indices = len(res[0]['model_outputs'])\n",
    "\n",
    "            index_correct_counts = [0] * num_indices\n",
    "            anycorrect_count = 0\n",
    "            self_consistency_count = 0\n",
    "\n",
    "            for r in res:\n",
    "                answer = r['entry']['answer']\n",
    "                preds = [extract_answer(mo) for mo in r['model_outputs']]\n",
    "\n",
    "                # (1) Index Accuracy\n",
    "                for i, pred in enumerate(preds):\n",
    "                    if pred == answer:\n",
    "                        index_correct_counts[i] += 1\n",
    "\n",
    "                # (2) Any Correct\n",
    "                if answer in preds:\n",
    "                    anycorrect_count += 1\n",
    "\n",
    "                # (3) Self-Consistency (majority voting)\n",
    "                pred_counter = Counter(preds)\n",
    "                majority_pred = pred_counter.most_common(1)[0][0]\n",
    "                if majority_pred == answer:\n",
    "                    self_consistency_count += 1\n",
    "\n",
    "            index_accuracy = [count / N for count in index_correct_counts]\n",
    "            anycorrect = anycorrect_count / N\n",
    "            self_consistency_accuracy = self_consistency_count / N\n",
    "\n",
    "            zero_result[subject] = {\n",
    "                \"N\": N,\n",
    "                \"index_accuracy\": index_accuracy,\n",
    "                \"self_consistency_accuracy\": self_consistency_accuracy,\n",
    "                \"anycorrect\": anycorrect,\n",
    "            }\n",
    "\n",
    "        # If we found no results at all for this model-type combination, skip\n",
    "        if not zero_result:\n",
    "            print(f\"\\n[INFO] No valid subjects for model={model}, type={t}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --------------------\n",
    "        # 2) Compute averages\n",
    "        # --------------------\n",
    "        total_len = 0\n",
    "        weighted_index_accuracy_sum = None\n",
    "        weighted_self_consistency_sum = 0\n",
    "        weighted_anycorrect_sum = 0\n",
    "\n",
    "        simple_index_accuracy_list = []\n",
    "        simple_self_consistency_list = []\n",
    "        simple_anycorrect_list = []\n",
    "\n",
    "        # Example subject to check how many indices exist\n",
    "        example_subject = next(iter(zero_result))\n",
    "        num_indices = len(zero_result[example_subject]['index_accuracy'])\n",
    "\n",
    "        for subj, result in zero_result.items():\n",
    "            N = result['N']\n",
    "            total_len += N\n",
    "\n",
    "            if weighted_index_accuracy_sum is None:\n",
    "                weighted_index_accuracy_sum = np.array(result['index_accuracy']) * N\n",
    "            else:\n",
    "                weighted_index_accuracy_sum += np.array(result['index_accuracy']) * N\n",
    "\n",
    "            weighted_self_consistency_sum += result['self_consistency_accuracy'] * N\n",
    "            weighted_anycorrect_sum += result['anycorrect'] * N\n",
    "\n",
    "            simple_index_accuracy_list.append(result['index_accuracy'])\n",
    "            simple_self_consistency_list.append(result['self_consistency_accuracy'])\n",
    "            simple_anycorrect_list.append(result['anycorrect'])\n",
    "\n",
    "        # Weighted average\n",
    "        if total_len > 0:\n",
    "            weighted_index_accuracy_avg = weighted_index_accuracy_sum / total_len\n",
    "            weighted_self_consistency_avg = weighted_self_consistency_sum / total_len\n",
    "            weighted_anycorrect_avg = weighted_anycorrect_sum / total_len\n",
    "        else:\n",
    "            weighted_index_accuracy_avg = [0] * num_indices\n",
    "            weighted_self_consistency_avg = 0.0\n",
    "            weighted_anycorrect_avg = 0.0\n",
    "\n",
    "        # Simple average (mean of each subject's accuracy)\n",
    "        simple_index_accuracy_avg = (\n",
    "            np.mean(simple_index_accuracy_list, axis=0) \n",
    "            if simple_index_accuracy_list \n",
    "            else [0] * num_indices\n",
    "        )\n",
    "        simple_self_consistency_avg = (\n",
    "            np.mean(simple_self_consistency_list) \n",
    "            if simple_self_consistency_list \n",
    "            else 0.0\n",
    "        )\n",
    "        simple_anycorrect_avg = (\n",
    "            np.mean(simple_anycorrect_list) \n",
    "            if simple_anycorrect_list \n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "        # --------------------\n",
    "        # 3) Print the table\n",
    "        # --------------------\n",
    "        print(f\"\\n=== Results for model={model}, type={t} ===\")\n",
    "\n",
    "        # Build table header\n",
    "        header = (\n",
    "            \"Subject\".ljust(25) +\n",
    "            \"len\".center(10) +\n",
    "            \"\".join([f\"Idx{i}\".center(10) for i in range(num_indices)]) +\n",
    "            \"Self-Consistency\".center(15) +\n",
    "            \"Anycorrect\".center(15)\n",
    "        )\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "        # Row for each subject\n",
    "        for subj, result in zero_result.items():\n",
    "            line = subj.ljust(25) + str(result['N']).center(10)\n",
    "            for acc in result['index_accuracy']:\n",
    "                line += f\"{acc*100:.2f}%\".center(10)\n",
    "            line += f\"{result['self_consistency_accuracy']*100:.2f}%\".center(15)\n",
    "            line += f\"{result['anycorrect']*100:.2f}%\".center(15)\n",
    "            print(line)\n",
    "\n",
    "        # Separator\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "        # Weighted average row\n",
    "        line_weighted = \"TOTAL (Weighted)\".ljust(25) + \"\".center(10)\n",
    "        for acc in weighted_index_accuracy_avg:\n",
    "            line_weighted += f\"{acc*100:.2f}%\".center(10)\n",
    "        line_weighted += f\"{weighted_self_consistency_avg*100:.2f}%\".center(15)\n",
    "        line_weighted += f\"{weighted_anycorrect_avg*100:.2f}%\".center(15)\n",
    "        print(line_weighted)\n",
    "\n",
    "        # Simple average row\n",
    "        line_simple = \"TOTAL (Simple)\".ljust(25) + \"\".center(10)\n",
    "        for acc in simple_index_accuracy_avg:\n",
    "            line_simple += f\"{acc*100:.2f}%\".center(10)\n",
    "        line_simple += f\"{simple_self_consistency_avg*100:.2f}%\".center(15)\n",
    "        line_simple += f\"{simple_anycorrect_avg*100:.2f}%\".center(15)\n",
    "        print(line_simple)\n",
    "\n",
    "        # Example of some additional metric\n",
    "        # (This looks like an MMLU-related metric: sum of index accuracy divided by 5, for 5 shots or 5 indices.)\n",
    "        print(f\"Some custom measure: {sum(weighted_index_accuracy_avg) / 5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing type: _zero_result ===\n",
      "\n",
      "=== Results for Llama (type: _zero_result) ===\n",
      "Subject                     len       Idx0      Idx1      Idx2      Idx3      Idx4   Self-Consistency   Anycorrect  \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "business                    300      35.00%    35.33%    32.00%    30.67%    34.67%       43.67%         65.33%    \n",
      "law                         300      25.33%    29.67%    26.33%    26.67%    28.00%       32.67%         61.00%    \n",
      "psychology                  300      59.67%    54.67%    58.00%    58.67%    57.00%       64.33%         81.67%    \n",
      "biology                     300      60.00%    63.33%    62.00%    61.33%    60.33%       68.33%         85.33%    \n",
      "chemistry                   300      23.67%    28.00%    26.00%    27.33%    27.33%       33.00%         63.33%    \n",
      "history                     300      42.33%    40.67%    40.67%    39.00%    39.67%       46.33%         69.00%    \n",
      "other                       300      42.67%    40.33%    43.33%    45.67%    41.67%       50.33%         74.33%    \n",
      "health                      300      57.00%    54.67%    55.00%    54.33%    53.67%       61.67%         81.67%    \n",
      "economics                   300      46.67%    47.67%    47.33%    48.33%    44.33%       53.00%         81.33%    \n",
      "math                        300      41.33%    38.33%    34.67%    39.33%    35.67%       47.67%         71.67%    \n",
      "physics                     300      31.00%    28.00%    28.67%    28.67%    27.33%       38.00%         66.33%    \n",
      "computer science            300      44.67%    40.33%    42.67%    40.67%    40.67%       50.33%         74.67%    \n",
      "philosophy                  300      38.67%    34.00%    35.00%    39.00%    37.67%       42.33%         68.33%    \n",
      "engineering                 300      24.67%    20.00%    22.00%    18.33%    20.00%       26.67%         58.33%    \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL (Weighted)                     40.90%    39.64%    39.55%    39.86%    39.14%       47.02%         71.60%    \n",
      "TOTAL (Simple)                       40.90%    39.64%    39.55%    39.86%    39.14%       47.02%         71.60%    \n",
      "Some custom measure: 0.3982\n",
      "\n",
      "=== Processing type: _result ===\n",
      "\n",
      "=== Results for Llama (type: _result) ===\n",
      "Subject                     len       Idx0      Idx1      Idx2      Idx3      Idx4   Self-Consistency   Anycorrect  \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "business                    300      39.00%    40.00%    39.33%    43.00%    41.00%       47.67%         71.67%    \n",
      "law                         300      27.67%    25.33%    29.00%    27.00%    28.67%       31.67%         62.33%    \n",
      "psychology                  300      55.67%    57.00%    53.33%    56.33%    54.67%       62.33%         77.33%    \n",
      "biology                     300      56.33%    58.00%    50.33%    55.67%    56.67%       68.67%         82.33%    \n",
      "chemistry                   300      25.33%    23.67%    20.67%    23.00%    21.67%       28.00%         60.00%    \n",
      "history                     300      42.67%    41.33%    43.33%    42.00%    44.67%       47.00%         66.67%    \n",
      "other                       300      41.00%    45.67%    43.67%    44.33%    43.33%       47.67%         72.00%    \n",
      "health                      300      45.00%    47.00%    48.67%    47.33%    48.00%       55.67%         77.00%    \n",
      "economics                   300      45.00%    46.00%    44.33%    44.33%    43.33%       51.33%         78.33%    \n",
      "math                        300      29.33%    32.67%    30.33%    32.67%    32.00%       38.67%         67.33%    \n",
      "physics                     300      26.00%    28.67%    26.67%    29.67%    29.67%       36.00%         65.67%    \n",
      "computer science            300      43.33%    44.00%    44.67%    42.67%    41.00%       50.33%         76.33%    \n",
      "philosophy                  300      36.00%    37.67%    39.67%    39.67%    39.33%       42.33%         68.00%    \n",
      "engineering                 300      23.67%    19.33%    21.67%    17.67%    20.67%       24.00%         56.33%    \n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL (Weighted)                     38.29%    39.02%    38.26%    38.95%    38.90%       45.10%         70.10%    \n",
      "TOTAL (Simple)                       38.29%    39.02%    38.26%    38.95%    38.90%       45.10%         70.10%    \n",
      "Some custom measure: 0.3869\n"
     ]
    }
   ],
   "source": [
    "llama_types = ['_zero_result', '_result']\n",
    "output_dir = \"../result/mmlu_pro/llama/\"\n",
    "\n",
    "\n",
    "for t in llama_types:\n",
    "    print(f\"\\n=== Processing type: {t} ===\")\n",
    "    llama_results = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_key = subject\n",
    "        \n",
    "\n",
    "        file_path = os.path.join(output_dir, subject_key + f\"{t}.json\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"[WARNING] File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not data:\n",
    "            print(f\"[WARNING] No predictions for subject={subject_key}\")\n",
    "            continue\n",
    "\n",
    "        N = len(data)\n",
    "        num_indices = 5  \n",
    "\n",
    "        index_correct_counts = [0] * num_indices\n",
    "        anycorrect_count = 0\n",
    "        self_consistency_count = 0\n",
    "\n",
    "        for entry in data:\n",
    "            answer = entry['doc']['gold']\n",
    "            \n",
    "            predictions = [extract_answer(model_output) for model_output in entry['resps'][0]]\n",
    "\n",
    "            \n",
    "            for i, pred in enumerate(predictions):\n",
    "                if pred == answer:\n",
    "                    index_correct_counts[i] += 1\n",
    "\n",
    "            \n",
    "            if answer in predictions:\n",
    "                anycorrect_count += 1\n",
    "\n",
    "            \n",
    "            majority_pred = Counter(predictions).most_common(1)[0][0]\n",
    "            if majority_pred == answer:\n",
    "                self_consistency_count += 1\n",
    "\n",
    "        index_accuracy = [count / N for count in index_correct_counts]\n",
    "        anycorrect = anycorrect_count / N\n",
    "        self_consistency_accuracy = self_consistency_count / N\n",
    "\n",
    "        llama_results[subject_key] = {\n",
    "            \"N\": N,\n",
    "            \"index_accuracy\": index_accuracy,\n",
    "            \"self_consistency_accuracy\": self_consistency_accuracy,\n",
    "            \"anycorrect\": anycorrect,\n",
    "        }\n",
    "\n",
    "    \n",
    "    if not llama_results:\n",
    "        print(f\"No valid subjects found for type {t}.\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    total_len = 0\n",
    "    weighted_index_accuracy_sum = None\n",
    "    weighted_self_consistency_sum = 0\n",
    "    weighted_anycorrect_sum = 0\n",
    "\n",
    "    simple_index_accuracy_list = []\n",
    "    simple_self_consistency_list = []\n",
    "    simple_anycorrect_list = []\n",
    "\n",
    "    \n",
    "    example_subject = next(iter(llama_results))\n",
    "    num_indices = len(llama_results[example_subject]['index_accuracy'])\n",
    "\n",
    "    for subj, result in llama_results.items():\n",
    "        N = result['N']\n",
    "        total_len += N\n",
    "\n",
    "        if weighted_index_accuracy_sum is None:\n",
    "            weighted_index_accuracy_sum = np.array(result['index_accuracy']) * N\n",
    "        else:\n",
    "            weighted_index_accuracy_sum += np.array(result['index_accuracy']) * N\n",
    "\n",
    "        weighted_self_consistency_sum += result['self_consistency_accuracy'] * N\n",
    "        weighted_anycorrect_sum += result['anycorrect'] * N\n",
    "\n",
    "        simple_index_accuracy_list.append(result['index_accuracy'])\n",
    "        simple_self_consistency_list.append(result['self_consistency_accuracy'])\n",
    "        simple_anycorrect_list.append(result['anycorrect'])\n",
    "\n",
    "    if total_len > 0:\n",
    "        weighted_index_accuracy_avg = weighted_index_accuracy_sum / total_len\n",
    "        weighted_self_consistency_avg = weighted_self_consistency_sum / total_len\n",
    "        weighted_anycorrect_avg = weighted_anycorrect_sum / total_len\n",
    "    else:\n",
    "        weighted_index_accuracy_avg = [0] * num_indices\n",
    "        weighted_self_consistency_avg = 0.0\n",
    "        weighted_anycorrect_avg = 0.0\n",
    "\n",
    "    simple_index_accuracy_avg = (\n",
    "        np.mean(simple_index_accuracy_list, axis=0)\n",
    "        if simple_index_accuracy_list\n",
    "        else [0] * num_indices\n",
    "    )\n",
    "    simple_self_consistency_avg = (\n",
    "        np.mean(simple_self_consistency_list)\n",
    "        if simple_self_consistency_list\n",
    "        else 0.0\n",
    "    )\n",
    "    simple_anycorrect_avg = (\n",
    "        np.mean(simple_anycorrect_list)\n",
    "        if simple_anycorrect_list\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"\\n=== Results for Llama (type: {t}) ===\")\n",
    "    header = (\n",
    "        \"Subject\".ljust(25) +\n",
    "        \"len\".center(10) +\n",
    "        \"\".join([f\"Idx{i}\".center(10) for i in range(num_indices)]) +\n",
    "        \"Self-Consistency\".center(15) +\n",
    "        \"Anycorrect\".center(15)\n",
    "    )\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for subj, result in llama_results.items():\n",
    "        line = subj.ljust(25) + str(result['N']).center(10)\n",
    "        for acc in result['index_accuracy']:\n",
    "            line += f\"{acc*100:.2f}%\".center(10)\n",
    "        line += f\"{result['self_consistency_accuracy']*100:.2f}%\".center(15)\n",
    "        line += f\"{result['anycorrect']*100:.2f}%\".center(15)\n",
    "        print(line)\n",
    "\n",
    "    print(\"-\" * len(header))\n",
    "    line_weighted = \"TOTAL (Weighted)\".ljust(25) + \"\".center(10)\n",
    "    for acc in weighted_index_accuracy_avg:\n",
    "        line_weighted += f\"{acc*100:.2f}%\".center(10)\n",
    "    line_weighted += f\"{weighted_self_consistency_avg*100:.2f}%\".center(15)\n",
    "    line_weighted += f\"{weighted_anycorrect_avg*100:.2f}%\".center(15)\n",
    "    print(line_weighted)\n",
    "\n",
    "    line_simple = \"TOTAL (Simple)\".ljust(25) + \"\".center(10)\n",
    "    for acc in simple_index_accuracy_avg:\n",
    "        line_simple += f\"{acc*100:.2f}%\".center(10)\n",
    "    line_simple += f\"{simple_self_consistency_avg*100:.2f}%\".center(15)\n",
    "    line_simple += f\"{simple_anycorrect_avg*100:.2f}%\".center(15)\n",
    "    print(line_simple)\n",
    "\n",
    "    \n",
    "    custom_metric = sum(weighted_index_accuracy_avg) / num_indices\n",
    "    print(f\"Some custom measure: {custom_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred(subject, model, output_dir):\n",
    "    likelihoods_file = os.path.join(output_dir, model, subject, \"few/all_likelihoods.json\")\n",
    "\n",
    "    if not os.path.exists(likelihoods_file):\n",
    "        print(f\"Error: {likelihoods_file} not found.\")\n",
    "        return\n",
    "    with open(likelihoods_file, \"r\") as f:\n",
    "        likelihoods = json.load(f)\n",
    "    problem_groups = list(zip(*likelihoods))\n",
    "\n",
    "    for problem_likelihoods in tqdm(problem_groups, desc=\"Processing problems\"):\n",
    "        problem_list = list(problem_likelihoods)\n",
    "        for cl in problem_list: \n",
    "            pred = extract_answer(cl['model_output'])\n",
    "            cl['pred'] = pred\n",
    "            is_correct = False\n",
    "            if pred == cl['answer']:\n",
    "                is_correct = True \n",
    "            cl['is_correct'] = is_correct\n",
    "\n",
    "    with open(likelihoods_file, \"w\") as f:\n",
    "        json.dump(likelihoods, f, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing problems: 100%|██████████| 300/300 [00:06<00:00, 49.48it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:05<00:00, 50.80it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:02<00:00, 138.28it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:11<00:00, 26.90it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:12<00:00, 23.71it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:05<00:00, 56.63it/s] \n",
      "Processing problems: 100%|██████████| 300/300 [00:04<00:00, 61.35it/s] \n",
      "Processing problems: 100%|██████████| 300/300 [00:12<00:00, 24.81it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:07<00:00, 37.94it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:06<00:00, 47.16it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:05<00:00, 51.00it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:05<00:00, 57.38it/s] \n",
      "Processing problems: 100%|██████████| 300/300 [00:07<00:00, 42.21it/s]\n",
      "Processing problems: 100%|██████████| 300/300 [00:17<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: llama ===\n",
      "Subject business: Accuracy = 0.4047\n",
      "Subject law: Accuracy = 0.2753\n",
      "Subject psychology: Accuracy = 0.5540\n",
      "Subject biology: Accuracy = 0.5540\n",
      "Subject chemistry: Accuracy = 0.2287\n",
      "Subject history: Accuracy = 0.4280\n",
      "Subject other: Accuracy = 0.4360\n",
      "Subject health: Accuracy = 0.4720\n",
      "Subject economics: Accuracy = 0.4460\n",
      "Subject math: Accuracy = 0.3140\n",
      "Subject physics: Accuracy = 0.2813\n",
      "Subject computer science: Accuracy = 0.4313\n",
      "Subject philosophy: Accuracy = 0.3847\n",
      "Subject engineering: Accuracy = 0.2060\n",
      "Overall Mean Accuracy (all subjects): 0.3869\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../likelihood/mmlu_pro\"\n",
    "\n",
    "for model in ['llama']:\n",
    "    total_correct = 0\n",
    "    total_entries = 0\n",
    "    subject_accuracies = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        add_pred(subject, model, base_dir)\n",
    "\n",
    "        likelihoods_file = os.path.join(base_dir, model, subject, \"few/all_likelihoods.json\")\n",
    "        if not os.path.exists(likelihoods_file):\n",
    "            print(f\"File not found: {likelihoods_file}\")\n",
    "            continue\n",
    "\n",
    "        with open(likelihoods_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            likelihoods = json.load(f)\n",
    "\n",
    "        subject_correct = 0\n",
    "        subject_total = 0\n",
    "        for sublist in likelihoods:\n",
    "            for cl in sublist:\n",
    "                subject_total += 1\n",
    "                if cl.get(\"is_correct\", False):\n",
    "                    subject_correct += 1\n",
    "\n",
    "        if subject_total > 0:\n",
    "            accuracy = subject_correct / subject_total\n",
    "        else:\n",
    "            accuracy = 0\n",
    "\n",
    "        subject_accuracies[subject] = accuracy\n",
    "        total_correct += subject_correct\n",
    "        total_entries += subject_total\n",
    "\n",
    "    if total_entries > 0:\n",
    "        overall_accuracy = total_correct / total_entries\n",
    "    else:\n",
    "        overall_accuracy = 0\n",
    "\n",
    "    print(f\"\\n=== Model: {model} ===\")\n",
    "    for subject, acc in subject_accuracies.items():\n",
    "        print(f\"Subject {subject}: Accuracy = {acc:.4f}\")\n",
    "    print(f\"Overall Mean Accuracy (all subjects): {overall_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
