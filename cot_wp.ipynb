{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from gpqa.gpqa_utils import * \n",
    "\n",
    "from math500.math_utils import * \n",
    "from math500.parser import *\n",
    "from math500.grader import * \n",
    "\n",
    "from mmlu_pro.mmlu_utils import * \n",
    "\n",
    "from hotpotqa.hotpotqa_utils import *\n",
    "\n",
    "from drop.drop_utils import *\n",
    "\n",
    "from musr.musr import MuSRDataset\n",
    "\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_answer_labels(answer_labels, tokenizer):\n",
    "    return tokenizer.convert_tokens_to_string(answer_labels)\n",
    "\n",
    "def get_token_spans(answer_labels, tokenizer):\n",
    "    full_text = decode_answer_labels(answer_labels, tokenizer)\n",
    "    spans = []\n",
    "    current_pos = 0\n",
    "    for token in answer_labels:\n",
    "        token_text = tokenizer.convert_tokens_to_string([token]).strip()\n",
    "        start = full_text.find(token_text, current_pos)\n",
    "        if start == -1:\n",
    "            start = current_pos\n",
    "        end = start + len(token_text)\n",
    "        spans.append((start, end))\n",
    "        current_pos = end\n",
    "    return full_text, spans\n",
    "\n",
    "def find_substring_token_indices(pred_text, full_text, token_spans):\n",
    "    start_char = full_text.rfind(pred_text)\n",
    "    if start_char == -1:\n",
    "        return -1, -1\n",
    "    end_char = start_char + len(pred_text)\n",
    "    \n",
    "    start_token = None\n",
    "    for i, (s, _) in enumerate(token_spans):\n",
    "        if s >= start_char:\n",
    "            start_token = i\n",
    "            break\n",
    "    if start_token is None:\n",
    "        return -1, -1\n",
    "    \n",
    "    end_token = None\n",
    "    for i in reversed(range(start_token, len(token_spans))):\n",
    "        _, e = token_spans[i]\n",
    "        if e <= end_char:\n",
    "            end_token = i\n",
    "            break\n",
    "    if end_token is None:\n",
    "        return -1, -1\n",
    "    return start_token, end_token\n",
    "\n",
    "\n",
    "def compute_prob_diff_for_token_range(start_idx, end_idx, topk_probs_list):\n",
    "    total_top1 = 0.0\n",
    "    total_top2 = 0.0\n",
    "    count = 0\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        if i >= len(topk_probs_list):\n",
    "            return None, None, None\n",
    "        probs = topk_probs_list[i]\n",
    "        if len(probs) < 2:\n",
    "            return None, None, None\n",
    "        total_top1 += probs[0]\n",
    "        total_top2 += probs[1]\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        return None, None, None\n",
    "    avg_top1 = total_top1 / count\n",
    "    avg_top2 = total_top2 / count\n",
    "    diff = avg_top1 - avg_top2\n",
    "    return avg_top1, avg_top2, diff\n",
    "\n",
    "\n",
    "def map_pred_to_prob_diff(pred, answer_labels, topk_probs_list, tokenizer):\n",
    "    full_text, token_spans = get_token_spans(answer_labels, tokenizer)\n",
    "    \n",
    "    pred_text = pred\n",
    "    full_text_norm = full_text\n",
    "    \n",
    "    if pred is None:\n",
    "        sentences = [s.strip() for s in re.split(r\"[.?!]\", full_text_norm) if s.strip()]\n",
    "        if sentences:\n",
    "            pred_text = sentences[-1]\n",
    "        else:\n",
    "            pred_text = full_text_norm\n",
    "    else:\n",
    "        pred_text = pred.lower().strip()\n",
    "\n",
    "    start_token, end_token = find_substring_token_indices(pred_text, full_text_norm, token_spans)\n",
    "    \n",
    "    if start_token == -1 or end_token == -1:\n",
    "        sentences = [s.strip() for s in re.split(r\"[.?!]\", full_text_norm) if s.strip()]\n",
    "        if sentences:\n",
    "            last_sentence = sentences[-1]\n",
    "        else:\n",
    "            last_sentence = full_text_norm  \n",
    "        \n",
    "        start_token, end_token = find_substring_token_indices(last_sentence, full_text_norm, token_spans)\n",
    "        if start_token == -1 or end_token == -1:\n",
    "            start_token = 0\n",
    "            end_token = len(answer_labels) - 1\n",
    "    \n",
    "    avg_top1, avg_top2, diff = compute_prob_diff_for_token_range(start_token, end_token, topk_probs_list)\n",
    "    if diff == None: \n",
    "        print(start_token, end_token)\n",
    "    return start_token, end_token, avg_top1, avg_top2, diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model = \"gpt-4o-mini\"  \n",
    "        self.task = \"math500\"  # \"math500\", \"mmlu_pro\", \"gpqa\", \"drop\", \"hotpotqa\"\n",
    "        self.shot_type = \"few\"  \n",
    "        self.output_dir = \"baselines/baseline\"\n",
    "        self.num_examples = -1       \n",
    "        self.subject = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/proj2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  \n",
    "\n",
    "tokenizer_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_eval = [\n",
    "    'max_prob_diff_output',\n",
    "    'total_max_prob_diff_output',\n",
    "]\n",
    "\n",
    "tasks = [\"math500\", \"mmlu_pro\", \"gpqa\", \"drop\", \"hotpotqa\", \"musr_location\", 'musr_efficiently']\n",
    "\n",
    "shots = [\"few\"]\n",
    "models = ['gpt-4o-mini', 'gpt-4o', 'llama']\n",
    "\n",
    "subjects = ['business', 'law', 'psychology', 'biology', 'chemistry', 'history', 'other', 'health', 'economics', 'math', 'physics', 'computer science', 'philosophy', 'engineering']\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation Results for model=gpt-4o-mini, shot=few =====\n",
      "max_prob_diff_output -> Accuracy: 0.7780\n",
      "total_max_prob_diff_output -> Accuracy: 0.7660\n",
      "--------------------------------------------------\n",
      "\n",
      "===== Evaluation Results for model=gpt-4o, shot=few =====\n",
      "max_prob_diff_output -> Accuracy: 0.7800\n",
      "total_max_prob_diff_output -> Accuracy: 0.7780\n",
      "--------------------------------------------------\n",
      "\n",
      "===== Evaluation Results for model=llama, shot=few =====\n",
      "max_prob_diff_output -> Accuracy: 0.4780\n",
      "total_max_prob_diff_output -> Accuracy: 0.4980\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from math500.math_utils import * \n",
    "from math500.parser import *\n",
    "from math500.grader import * \n",
    "\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"{config.output_dir}/math500/{model}/math500_{config.shot_type}_few.jsonl\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "\n",
    "        scores = {k: [] for k in keys_to_eval}\n",
    "\n",
    "        for entry in data:\n",
    "            idx = entry[\"idx\"]\n",
    "        \n",
    "            _, gt = parse_ground_truth(entry['entry'], \"math\")\n",
    "\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            results_info = entry.get('results', [])\n",
    "\n",
    "\n",
    "            best_total_diff = float('-inf')\n",
    "            best_total_pred = None\n",
    "            best_total_mo = None\n",
    "            best_total_em, best_total_f1 = 0.0, 0.0\n",
    "\n",
    "            best_diff = float('-inf')\n",
    "            best_pred = None\n",
    "            best_mo = None\n",
    "            best_em, best_f1 = 0.0, 0.0\n",
    "\n",
    "\n",
    "            for i, mo in enumerate(model_outputs):\n",
    "\n",
    "                r = results_info[i] if i < len(results_info) else {}\n",
    "                answer_labels = r.get(\"answer_labels\", [])\n",
    "                answer_label_probs = r.get(\"answer_label_probs\", [])\n",
    "                topk_tokens_list = r.get(\"topk_tokens\", [])\n",
    "                topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "                total_diff = np.mean([top1 - top2 for top1, top2 in topk_probs_list])\n",
    "\n",
    "                \n",
    "                pred = extract_answer(mo, \"math\")\n",
    "                start_idx, end_idx, avg_top1, avg_top2, diff = map_pred_to_prob_diff(\n",
    "                    pred,\n",
    "                    answer_labels,\n",
    "                    topk_probs_list,\n",
    "                    tokenizer\n",
    "                )\n",
    "\n",
    "                pred = strip_string(pred)\n",
    "\n",
    "                if total_diff > best_total_diff:\n",
    "                    best_total_diff = total_diff\n",
    "                    best_total_pred = pred\n",
    "                    best_total_mo = mo \n",
    "\n",
    "\n",
    "                if diff > best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_pred = pred\n",
    "                    best_mo = mo\n",
    "                    \n",
    "\n",
    "            k = \"max_prob_diff_output\"\n",
    "\n",
    "            \n",
    "            result = math_equal_process((idx, best_pred, gt))\n",
    "        \n",
    "            if not result:\n",
    "                result = process_results(gt, [best_mo])\n",
    "                if not result:\n",
    "                    best_pred = extract_answer(best_pred, \"math\")\n",
    "                    result = math_equal_process((None, best_pred, gt))\n",
    "            scores[k].append(result)\n",
    "\n",
    "\n",
    "            k = \"total_max_prob_diff_output\"\n",
    "\n",
    "            \n",
    "            result = math_equal_process((idx, best_total_pred, gt))\n",
    "\n",
    "            if not result :\n",
    "                result = process_results(gt, [best_total_mo])\n",
    "                if not result:\n",
    "                    best_total_pred = extract_answer(best_total_pred, \"math\")\n",
    "                    result = math_equal_process((None, best_total_pred, gt))\n",
    "            scores[k].append(result)\n",
    "\n",
    "\n",
    "        print(f\"\\n===== Evaluation Results for model={model}, shot={config.shot_type} =====\")\n",
    "        for key in keys_to_eval:\n",
    "\n",
    "            if len(scores[key]) == 0:\n",
    "                print(f\"{key} -> No data / Not found in entries\")\n",
    "                continue\n",
    "\n",
    "            acc = sum(scores[key]) / len(scores[key])\n",
    "            print(f\"{key} -> Accuracy: {acc:.4f}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Results for model=gpt-4o-mini ===\n",
      "max_prob_diff_output -> Accuracy: 0.6417\n",
      "total_max_prob_diff_output -> Accuracy: 0.6469\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Overall Results for model=gpt-4o ===\n",
      "max_prob_diff_output -> Accuracy: 0.7500\n",
      "total_max_prob_diff_output -> Accuracy: 0.7517\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Overall Results for model=llama ===\n",
      "max_prob_diff_output -> Accuracy: 0.4483\n",
      "total_max_prob_diff_output -> Accuracy: 0.4507\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_answer(text):\n",
    "    pattern = r\"answer is \\(?([A-J])\\)?\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return extract_again(text)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    overall_scores = {k: 0 for k in keys_to_eval}  \n",
    "    overall_total_entries = 0\n",
    "\n",
    "    for subject in subjects:\n",
    "        file_path = f\"{config.output_dir}/mmlu_pro/{model}/{subject}/mmlu_pro_few_few.jsonl\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "        subject_scores = {k: 0 for k in keys_to_eval}\n",
    "        total_data_len = len(data)\n",
    "        overall_total_entries += total_data_len\n",
    "\n",
    "        for entry in data:\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            results_info = entry.get('results', [])\n",
    "            answer = entry['entry'].get('answer') or entry['entry'].get('gold')\n",
    "\n",
    "\n",
    "            best_diff = float('-inf')\n",
    "            best_pred = None\n",
    "\n",
    "            best_total_diff = float('-inf')\n",
    "            best_total_pred = None\n",
    "\n",
    "\n",
    "            for i, mo in enumerate(model_outputs):\n",
    "                r = results_info[i] if i < len(results_info) else {}\n",
    "                answer_labels = r.get(\"answer_labels\", [])\n",
    "                answer_label_probs = r.get(\"answer_label_probs\", [])\n",
    "                topk_tokens_list = r.get(\"topk_tokens\", [])\n",
    "                topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "\n",
    "\n",
    "                total_diff = np.mean([top1 - top2 for top1, top2 in topk_probs_list])\n",
    "\n",
    "                pred = extract_answer(mo)\n",
    "                if pred is None:\n",
    "                    sentences = [s.strip() for s in re.split(r\"[.?!]\", mo) if s.strip()]\n",
    "                    pred = sentences[-1] if sentences else mo\n",
    "\n",
    "                start_idx, end_idx, avg_top1, avg_top2, diff = map_pred_to_prob_diff(\n",
    "                    pred,\n",
    "                    answer_labels,\n",
    "                    topk_probs_list,\n",
    "                    tokenizer\n",
    "                )\n",
    "\n",
    "                if total_diff > best_total_diff:\n",
    "                    best_total_diff = total_diff\n",
    "                    best_total_pred = pred\n",
    "\n",
    "                if diff > best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_pred = pred\n",
    "\n",
    "\n",
    "            k = \"max_prob_diff_output\"\n",
    "            if best_pred == answer:\n",
    "                subject_scores[k] += 1\n",
    "            k = \"total_max_prob_diff_output\"\n",
    "            if best_total_pred == answer:\n",
    "                subject_scores[k] += 1\n",
    "\n",
    "        for k in keys_to_eval:\n",
    "            overall_scores[k] += subject_scores[k]\n",
    "\n",
    "    print(f\"\\n=== Overall Results for model={model} ===\")\n",
    "    for key in keys_to_eval:\n",
    "        if overall_total_entries == 0:\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = overall_scores[key] / overall_total_entries\n",
    "        print(f\"{key} -> Accuracy: {acc:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for model=gpt-4o-mini, shot=few ===\n",
      "max_prob_diff_output -> Accuracy: 0.4242\n",
      "total_max_prob_diff_output -> Accuracy: 0.4293\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Results for model=gpt-4o, shot=few ===\n",
      "max_prob_diff_output -> Accuracy: 0.4848\n",
      "total_max_prob_diff_output -> Accuracy: 0.5253\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Results for model=llama, shot=few ===\n",
      "max_prob_diff_output -> Accuracy: 0.3232\n",
      "total_max_prob_diff_output -> Accuracy: 0.3384\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from gpqa.gpqa_utils import * \n",
    "\n",
    "examples = load_examples(\"data/gpqa/gpqa_diamond.csv\", seed=0)\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"{config.output_dir}/gpqa/{model}/gpqa_few_{config.shot_type}.jsonl\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    scores = {k: 0 for k in keys_to_eval}\n",
    "\n",
    "\n",
    "    total_data_len = len(data)\n",
    "    if total_data_len != len(examples):\n",
    "        print(\"Warning: data length and examples length do not match!\")\n",
    "\n",
    "    for entry, example in zip(data, examples):\n",
    "        correct_index = example.correct_index  \n",
    "\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        results_info = entry.get('results', [])\n",
    "\n",
    "\n",
    "        best_total_diff = float('-inf')\n",
    "        best_total_pred = None\n",
    "        best_total_mo = None\n",
    "        best_total_em, best_total_f1 = 0.0, 0.0\n",
    "        \n",
    "        best_diff = float('-inf')\n",
    "        best_pred = None\n",
    "        best_em, best_f1 = 0.0, 0.0\n",
    "\n",
    "\n",
    "        for i, mo in enumerate(model_outputs):\n",
    "            pred = parse_sampled_answer(mo)\n",
    "            if pred is None:\n",
    "                pred = mo\n",
    "\n",
    "\n",
    "            r = results_info[i] if i < len(results_info) else {}\n",
    "            answer_labels = r.get(\"answer_labels\", [])\n",
    "            answer_label_probs = r.get(\"answer_label_probs\", [])\n",
    "            topk_tokens_list = r.get(\"topk_tokens\", [])\n",
    "            topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "            total_diff = np.mean([top1 - top2 for top1, top2 in topk_probs_list])\n",
    "\n",
    "\n",
    "            start_idx, end_idx, avg_top1, avg_top2, diff = map_pred_to_prob_diff(\n",
    "                pred,\n",
    "                answer_labels,\n",
    "                topk_probs_list,\n",
    "                tokenizer\n",
    "            )\n",
    "\n",
    "            if total_diff > best_total_diff:\n",
    "                best_total_diff = total_diff\n",
    "                best_total_pred = pred\n",
    "                best_total_mo = mo \n",
    "\n",
    "\n",
    "            if diff > best_diff:\n",
    "                best_diff = diff\n",
    "                best_pred = pred\n",
    "                \n",
    "\n",
    "        k = \"max_prob_diff_output\"\n",
    "\n",
    "        if best_pred is None or len(best_pred) > 5:\n",
    "            is_correct = False\n",
    "        else:\n",
    "\n",
    "            is_correct = (LETTER_TO_INDEX[best_pred] == correct_index)\n",
    "        scores[k] += int(is_correct)\n",
    "\n",
    "        k = \"total_max_prob_diff_output\"\n",
    "\n",
    "        if best_total_pred is None or len(best_total_pred) > 5:\n",
    "            is_correct = False\n",
    "        else:\n",
    "\n",
    "            is_correct = (LETTER_TO_INDEX[best_total_pred] == correct_index)\n",
    "        scores[k] += int(is_correct)\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Results for model={model}, shot={config.shot_type} ===\")\n",
    "    for key in keys_to_eval:\n",
    "        acc = scores[key] / total_data_len if total_data_len else 0\n",
    "        print(f\"{key} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "\n",
      "===== Results for model=gpt-4o-mini, shot=few =====\n",
      "max_prob_diff_output -> EM: 0.7760, F1: 0.8307\n",
      "total_max_prob_diff_output -> EM: 0.7640, F1: 0.8275\n",
      "--------------------------------------------------\n",
      "gpt-4o\n",
      "\n",
      "===== Results for model=gpt-4o, shot=few =====\n",
      "max_prob_diff_output -> EM: 0.8300, F1: 0.9081\n",
      "total_max_prob_diff_output -> EM: 0.8100, F1: 0.8934\n",
      "--------------------------------------------------\n",
      "llama\n",
      "\n",
      "===== Results for model=llama, shot=few =====\n",
      "max_prob_diff_output -> EM: 0.7020, F1: 0.7510\n",
      "total_max_prob_diff_output -> EM: 0.6960, F1: 0.7516\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from drop.drop_utils import *\n",
    "\n",
    "for model in models: \n",
    "    print(model)\n",
    "    for shot_type in ['few']:\n",
    "        file_path = f\"{config.output_dir}/drop/{model}/drop_few_{shot_type}.jsonl\"\n",
    "    \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "        \n",
    "        em_scores = {k: [] for k in keys_to_eval}\n",
    "        f1_scores = {k: [] for k in keys_to_eval}\n",
    "\n",
    "\n",
    "        def get_max_em_f1(pred, golds):\n",
    "            max_em, max_f1 = 0.0, 0.0\n",
    "            for gold_answer in golds:\n",
    "                exact_match, f1_score = get_metrics(pred, gold_answer)\n",
    "\n",
    "                if gold_answer[0].strip():\n",
    "                    max_em = max(max_em, exact_match)\n",
    "                    max_f1 = max(max_f1, f1_score)\n",
    "            return max_em, max_f1\n",
    "\n",
    "\n",
    "        for entry in data:\n",
    "            golds = get_answers(entry['entry'])  # gold answers\n",
    "\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            results_info = entry.get('results', [])\n",
    "\n",
    "\n",
    "            best_total_diff = float('-inf')\n",
    "            best_total_pred = None\n",
    "            best_total_mo = None\n",
    "            best_total_em, best_total_f1 = 0.0, 0.0\n",
    "\n",
    "            best_diff = float('-inf')\n",
    "            best_pred = None\n",
    "            best_em, best_f1 = 0.0, 0.0\n",
    "\n",
    "\n",
    "            for i, mo in enumerate(model_outputs):\n",
    "                pred = extract_answer(mo)\n",
    "\n",
    "                r = results_info[i] if i < len(results_info) else {}\n",
    "                answer_labels = r.get(\"answer_labels\", [])\n",
    "                answer_label_probs = r.get(\"answer_label_probs\", [])\n",
    "                topk_tokens_list = r.get(\"topk_tokens\", [])\n",
    "                topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "                total_diff = np.mean([top1 - top2 for top1, top2 in topk_probs_list])\n",
    "\n",
    "\n",
    "                start_idx, end_idx, avg_top1, avg_top2, diff = map_pred_to_prob_diff(\n",
    "                    pred,\n",
    "                    answer_labels,\n",
    "                    topk_probs_list,\n",
    "                    tokenizer\n",
    "                )\n",
    "\n",
    "                if total_diff > best_total_diff:\n",
    "                    best_total_diff = diff\n",
    "                    best_total_pred = pred\n",
    "\n",
    "                    em_val, f1_val = get_max_em_f1(pred, golds)\n",
    "                    best_total_em, best_total_f1 = em_val, f1_val\n",
    "\n",
    "\n",
    "                if diff > best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_pred = pred\n",
    "\n",
    "                    em_val, f1_val = get_max_em_f1(pred, golds)\n",
    "                    best_em, best_f1 = em_val, f1_val\n",
    "\n",
    "\n",
    "\n",
    "            k = \"max_prob_diff_output\"\n",
    "            em_scores[k].append(best_em)\n",
    "            f1_scores[k].append(best_f1)\n",
    "\n",
    "            k = \"total_max_prob_diff_output\"\n",
    "            em_scores[k].append(best_total_em)\n",
    "            f1_scores[k].append(best_total_f1)\n",
    "            \n",
    "\n",
    "        print(f\"\\n===== Results for model={model}, shot={shot_type} =====\")\n",
    "        for k in keys_to_eval:\n",
    "            if len(em_scores[k]) == 0:\n",
    "                print(f\"{k}: No entries found, skip.\")\n",
    "                continue\n",
    "\n",
    "            em_mean = np.mean(em_scores[k])\n",
    "            f1_mean = np.mean(f1_scores[k])\n",
    "            print(f\"{k} -> EM: {em_mean:.4f}, F1: {f1_mean:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "\n",
      "=== Results for model=gpt-4o-mini, shot=few ===\n",
      "max_prob_diff_output -> EM: 0.3400, F1: 0.4564\n",
      "total_max_prob_diff_output -> EM: 0.3620, F1: 0.4770\n",
      "--------------------------------------------------\n",
      "gpt-4o\n",
      "\n",
      "=== Results for model=gpt-4o, shot=few ===\n",
      "max_prob_diff_output -> EM: 0.4800, F1: 0.6122\n",
      "total_max_prob_diff_output -> EM: 0.4600, F1: 0.6037\n",
      "--------------------------------------------------\n",
      "llama\n",
      "\n",
      "=== Results for model=llama, shot=few ===\n",
      "max_prob_diff_output -> EM: 0.2500, F1: 0.3221\n",
      "total_max_prob_diff_output -> EM: 0.2440, F1: 0.3175\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from hotpotqa.hotpotqa_utils import *\n",
    "\n",
    "def extract_answer(response_text):\n",
    "    match = re.search(r\"Answer\\s+(.+)\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "\n",
    "        answer = re.sub(r\"[.\\n]+$\", \"\", answer).strip()\n",
    "        return answer\n",
    "\n",
    "\n",
    "    match = re.search(r\"(?<!\\w)Answer[:\\s]+(.+?)(?:[.\\n]|$)\", response_text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "\n",
    "        answer = re.sub(r\"[.\\n]+$\", \"\", answer).strip()\n",
    "        return answer\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "dataset = json.load(open(f'data/hotpotqa/hotpotqa.json'))\n",
    "with open(\"hotpotqa/react_prompt.json\", 'r') as f:\n",
    "    fewshot = json.load(f)\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for shot_type in ['few']:\n",
    "        file_path = f\"{config.output_dir}/hotpotqa/{model}/hotpotqa_few_{shot_type}.jsonl\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "        \n",
    "\n",
    "        preds = {k: [] for k in keys_to_eval}\n",
    "\n",
    "        for entry in data:\n",
    "\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            results_info = entry.get('results', [])\n",
    "\n",
    "\n",
    "            best_total_diff = float('-inf')\n",
    "            best_total_pred = None\n",
    "            best_total_mo = None\n",
    "            best_total_em, best_total_f1 = 0.0, 0.0\n",
    "\n",
    "            best_diff = float('-inf')\n",
    "            best_pred = None\n",
    "            best_em, best_f1 = 0.0, 0.0\n",
    "\n",
    "            for i, mo in enumerate(model_outputs):\n",
    "                pred = extract_answer(mo)\n",
    "\n",
    "                r = results_info[i] if i < len(results_info) else {}\n",
    "                answer_labels = r.get(\"answer_labels\", [])\n",
    "                answer_label_probs = r.get(\"answer_label_probs\", [])\n",
    "                topk_tokens_list = r.get(\"topk_tokens\", [])\n",
    "                topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "\n",
    "                total_diff = np.mean([top1 - top2 for top1, top2 in topk_probs_list])\n",
    "\n",
    "\n",
    "                start_idx, end_idx, avg_top1, avg_top2, diff = map_pred_to_prob_diff(\n",
    "                    pred,\n",
    "                    answer_labels,\n",
    "                    topk_probs_list,\n",
    "                    tokenizer\n",
    "                )\n",
    "\n",
    "                if total_diff > best_total_diff:\n",
    "                    best_total_diff = total_diff\n",
    "                    best_total_pred = pred\n",
    "                    best_total_mo = mo \n",
    "\n",
    "\n",
    "                if diff > best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_pred = pred\n",
    "                    \n",
    "\n",
    "            k = \"total_max_prob_diff_output\"\n",
    "            preds[k].append(best_total_pred)\n",
    "\n",
    "\n",
    "            k = \"max_prob_diff_output\"\n",
    "            preds[k].append(best_pred)\n",
    "\n",
    "            \n",
    "        print(f\"\\n=== Results for model={model}, shot={shot_type} ===\")\n",
    "        for k in keys_to_eval:\n",
    "            if len(preds[k]) == 0:\n",
    "                print(f\"{k}: No entries found, skip.\")\n",
    "                continue\n",
    "\n",
    "            em_scores, f1_scores = get_em_f1(dataset, preds[k])\n",
    "            em_mean = em_scores.mean()\n",
    "            f1_mean = f1_scores.mean()\n",
    "            print(f\"{k} -> EM: {em_mean:.4f}, F1: {f1_mean:.4f}\")\n",
    "        \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musr.musr import MuSRDataset\n",
    "\n",
    "mm_path = 'data/musr/murder_mystery.json'\n",
    "mm = MuSRDataset(mm_path)\n",
    "\n",
    "ta_path = 'data/musr/team_allocation.json'\n",
    "ta = MuSRDataset(ta_path)\n",
    "\n",
    "op_path = 'data/musr/object_placements.json'\n",
    "op = MuSRDataset(op_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    for shot_type in ['few']:\n",
    "\n",
    "        file_path = f\"{config.output_dir}/musr_location/{model}/musr_location_few_{shot_type}.jsonl\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "        \n",
    "        preds = {k: [] for k in keys_to_eval}\n",
    "\n",
    "        for test_idx, entry in enumerate(data):\n",
    "\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            results_info = entry.get('results', [])\n",
    "\n",
    "\n",
    "            best_total_diff = float('-inf')\n",
    "            best_total_pred = None\n",
    "\n",
    "\n",
    "            best_diff = float('-inf')\n",
    "            best_pred = None\n",
    "\n",
    "            for i, mo in enumerate(model_outputs):\n",
    "\n",
    "                pred = op.evaluate_response([mo], op[test_idx])[0]['model_answer']\n",
    "                \n",
    "                r = results_info[i] if i < len(results_info) else {}\n",
    "                topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "\n",
    "\n",
    "                total_diff = np.mean([top1 - top2 for (top1, top2) in topk_probs_list]) \\\n",
    "                            if topk_probs_list else float('-inf')\n",
    "\n",
    "\n",
    "                _, _, _, _, diff = map_pred_to_prob_diff(\n",
    "                    pred,\n",
    "                    r.get(\"answer_labels\", []),\n",
    "                    topk_probs_list,\n",
    "                    tokenizer\n",
    "                )\n",
    "\n",
    "\n",
    "                if total_diff > best_total_diff:\n",
    "                    best_total_diff = total_diff\n",
    "                    best_total_pred = mo\n",
    "\n",
    "\n",
    "                if diff > best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_pred = mo\n",
    "\n",
    "\n",
    "            preds[\"total_max_prob_diff_output\"].append(best_total_pred)\n",
    "            preds[\"max_prob_diff_output\"].append(best_pred)\n",
    "\n",
    "\n",
    "        total_data_len = len(data)        \n",
    "        scores = {k: 0 for k in keys_to_eval}\n",
    "        \n",
    "        for i, entry in enumerate(data):\n",
    "            if 'entry' not in entry:\n",
    "                continue\n",
    "            \n",
    "            for k in keys_to_eval:\n",
    "\n",
    "                pred_answer = preds[k][i]\n",
    "                if pred_answer is not None:\n",
    "                    metrics = op.evaluate_response([pred_answer], op[i])\n",
    "                    if metrics and metrics[0]['correct']:\n",
    "                        scores[k] += 1\n",
    "\n",
    "        print(f\"\\n=== Results for model={model}, shot={shot_type} ===\")\n",
    "        for k in keys_to_eval:\n",
    "\n",
    "            if len(preds[k]) == 0:\n",
    "                print(f\"{k}: No entries found, skip.\")\n",
    "                continue\n",
    "\n",
    "            acc = scores[k] / total_data_len if total_data_len else 0\n",
    "            print(f\"{k} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ['gpt-4o-mini', 'gpt-4o', 'llama']:\n",
    "    print(model)\n",
    "    for shot_type in ['few']:\n",
    "\n",
    "        file_path = f\"{config.output_dir}/musr_efficiently/{model}/musr_efficiently_few_{shot_type}.jsonl\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = [json.loads(line) for line in f]\n",
    "        \n",
    "        preds = {k: [] for k in keys_to_eval}\n",
    "\n",
    "        for test_idx, entry in enumerate(data):\n",
    "\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            results_info = entry.get('results', [])\n",
    "\n",
    "            best_total_diff = float('-inf')\n",
    "            best_total_pred = None\n",
    "\n",
    "            best_diff = float('-inf')\n",
    "            best_pred = None\n",
    "\n",
    "            for i, mo in enumerate(model_outputs):\n",
    "                pred = ta.evaluate_response([mo], ta[test_idx])[0]['model_answer']\n",
    "                \n",
    "                r = results_info[i] if i < len(results_info) else {}\n",
    "                topk_probs_list  = r.get(\"topk_probs\", [])\n",
    "\n",
    "\n",
    "                total_diff = np.mean([top1 - top2 for (top1, top2) in topk_probs_list]) \\\n",
    "                            if topk_probs_list else float('-inf')\n",
    "\n",
    "\n",
    "                _, _, _, _, diff = map_pred_to_prob_diff(\n",
    "                    pred,\n",
    "                    r.get(\"answer_labels\", []),\n",
    "                    topk_probs_list,\n",
    "                    tokenizer\n",
    "                )\n",
    "\n",
    "                if total_diff > best_total_diff:\n",
    "                    best_total_diff = total_diff\n",
    "                    best_total_pred = mo\n",
    "\n",
    "\n",
    "                if diff > best_diff:\n",
    "                    best_diff = diff\n",
    "                    best_pred = mo\n",
    "\n",
    "            preds[\"total_max_prob_diff_output\"].append(best_total_pred)\n",
    "            preds[\"max_prob_diff_output\"].append(best_pred)\n",
    "\n",
    "        total_data_len = len(data)\n",
    "        \n",
    "        scores = {k: 0 for k in keys_to_eval}\n",
    "    \n",
    "        for i, entry in enumerate(data):\n",
    "            if 'entry' not in entry:\n",
    "                continue\n",
    "            \n",
    "            for k in keys_to_eval:\n",
    "\n",
    "                pred_answer = preds[k][i]\n",
    "                if pred_answer is not None:\n",
    "                    metrics = ta.evaluate_response([pred_answer], ta[i])\n",
    "                    if metrics and metrics[0]['correct']:\n",
    "                        scores[k] += 1\n",
    "\n",
    "\n",
    "        print(f\"\\n=== Results for model={model}, shot={shot_type} ===\")\n",
    "        for k in keys_to_eval:\n",
    "\n",
    "            if len(preds[k]) == 0:\n",
    "                print(f\"{k}: No entries found, skip.\")\n",
    "                continue\n",
    "\n",
    "            acc = scores[k] / total_data_len if total_data_len else 0\n",
    "            print(f\"{k} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
