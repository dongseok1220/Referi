{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehdtjr1220/miniconda3/envs/proj2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "import sys \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "sys.path.append('../')  \n",
    "\n",
    "from gpqa.gpqa_utils import * \n",
    "\n",
    "from math500.math_utils import * \n",
    "from math500.parser import *\n",
    "from math500.grader import * \n",
    "\n",
    "from mmlu_pro.mmlu_utils import * \n",
    "\n",
    "from hotpotqa.hotpotqa_utils import *\n",
    "\n",
    "from drop.drop_utils import *\n",
    "\n",
    "from utils import load_model_outputs\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def construct_prompt(args): \n",
    "    if args.task != 'mmlu_pro': \n",
    "        output_res_path = f\"../result/{args.task}/{args.model}/{args.task}_{args.shot_type}.jsonl\"\n",
    "        res = load_model_outputs(output_res_path)\n",
    "        \n",
    "    system_prompt = \"\"\n",
    "    start_prompt = \"I have generated the following responses to the question: \"\n",
    "    end_prompt = \"\"\"\\n\\nEvaluate these responses.\\nSelect the most consistent response based on majority consensus.\\nStart your answer with \"The most consistent response is Response X\" (without quotes).\"\"\"\n",
    "\n",
    "    samples = []\n",
    "    if args.task == \"math500\":\n",
    "        for idx, r in tqdm(enumerate(res)):\n",
    "            entry = r.get('entry', {})\n",
    "            question = entry.get('problem', '')\n",
    "            model_outputs = r.get('model_outputs', [])\n",
    "\n",
    "            user_prompt = start_prompt + f\"{question}\\n\\n\"\n",
    "            #\n",
    "            model_outputs = model_outputs[::-1]\n",
    "            #\n",
    "            \n",
    "            for i, output in enumerate(model_outputs):\n",
    "                user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "            user_prompt += end_prompt\n",
    "\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "            samples.append(sample)\n",
    "        \n",
    "    elif args.task == \"mmlu_pro\":\n",
    "        def format_example(question, options, cot_content=\"\"):\n",
    "            example = \"{}\\nOptions: \".format(question)\n",
    "            choice_map = \"ABCDEFGHIJ\"\n",
    "            for i, opt in enumerate(options):\n",
    "                example += \"{}. {}\\n\".format(choice_map[i], opt)\n",
    "            \n",
    "            return example\n",
    "        \n",
    "        dataset, fewshot = load_mmlu_pro()\n",
    "        subjects = list(dataset.keys())\n",
    "        for subject in tqdm(subjects): \n",
    "            res_path = f\"../result/{args.task}/{args.model}/{subject}_result.jsonl\"\n",
    "            res = load_model_outputs(res_path)\n",
    "\n",
    "            for idx, r in tqdm(enumerate(res)):\n",
    "                entry = r.get('entry', {})\n",
    "                model_outputs = r.get('model_outputs', [])\n",
    "                question = format_example(entry['question'], entry['options'])\n",
    "\n",
    "                user_prompt = start_prompt + f\"{question}\\n\\n\"\n",
    "                for i, output in enumerate(model_outputs):\n",
    "                    user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "                user_prompt += end_prompt\n",
    "\n",
    "                message = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "                sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "                samples.append(sample)\n",
    "    \n",
    "    elif args.task == \"gpqa\": \n",
    "        def format_example(example) -> str:\n",
    "            prompt = f\"Question: {example[0]}\"\n",
    "            prompt += f\"\\nChoices:\\n(A) {example[1]}\\n(B) {example[2]}\\n(C) {example[3]}\\n(D) {example[4]}\"\n",
    "            return prompt\n",
    "        \n",
    "        for idx, r in tqdm(enumerate(res)):\n",
    "            entry = r.get('entry', {})\n",
    "            model_outputs = r.get('model_outputs', [])\n",
    "            question = format_example(entry)\n",
    "\n",
    "            user_prompt = start_prompt + f\"{question}\\n\\n\"\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "            user_prompt += end_prompt\n",
    "\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "            samples.append(sample)\n",
    "\n",
    "    elif args.task == \"hotpotqa\": \n",
    "        for idx, r in tqdm(enumerate(res)):\n",
    "            entry = r.get('entry', {})\n",
    "            model_outputs = r.get('model_outputs', [])\n",
    "            question = entry['question']\n",
    "\n",
    "            user_prompt = start_prompt + f\"{question}\\n\\n\"\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "            user_prompt += end_prompt\n",
    "\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "            samples.append(sample)\n",
    "\n",
    "    elif args.task == \"drop\": \n",
    "         for idx, r in tqdm(enumerate(res)):\n",
    "            entry = r.get('entry', {})\n",
    "            model_outputs = r.get('model_outputs', [])\n",
    "            \n",
    "            user_prompt = start_prompt + f\"{entry['passage']} {entry['question']}\\n\\n\" \n",
    "            for i, output in enumerate(model_outputs):\n",
    "                user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "            user_prompt += end_prompt\n",
    "            \n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "            samples.append(sample)\n",
    "\n",
    "    elif args.task == \"musr_location\": \n",
    "        for idx, r in tqdm(enumerate(res)):\n",
    "            entry = r.get('entry', {})\n",
    "            model_outputs = r.get('model_outputs', [])\n",
    "\n",
    "            question = entry['question'].strip()\n",
    "            context = entry['context'].strip()\n",
    "            choices = entry['choices']['text']\n",
    "            \n",
    "            labels = ['A', 'B', 'C', 'D', 'E', 'F'][:len(choices)]\n",
    "            choice_str = '\\n'.join([f'{labels[idx]}: {choices[idx]}' for idx in range(len(choices))])\n",
    "            original_question_part = f\"{context}\\n\\n{question}\\n\\n{choice_str}\"\n",
    "                        \n",
    "            user_prompt = start_prompt + f\"{original_question_part}\\n\\n\" \n",
    "            for i, output in enumerate(model_outputs):\n",
    "                user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "            user_prompt += end_prompt\n",
    "            \n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "            samples.append(sample)\n",
    "        \n",
    "    elif args.task == \"musr_efficiently\":\n",
    "        for idx, r in tqdm(enumerate(res)):\n",
    "            entry = r.get('entry', {})\n",
    "            model_outputs = r.get('model_outputs', [])\n",
    "\n",
    "            question = entry['question'].strip()\n",
    "            context = entry['context'].strip()\n",
    "            choices = entry['choices']['text']\n",
    "            \n",
    "            labels = ['A', 'B', 'C', 'D', 'E', 'F'][:len(choices)]\n",
    "            choice_str = '\\n'.join([f'{labels[idx]}: {choices[idx]}' for idx in range(len(choices))])\n",
    "            original_question_part = f\"{context}\\n\\n{question}\\n\\n{choice_str}\"\n",
    "                        \n",
    "            user_prompt = start_prompt + f\"{original_question_part}\\n\\n\" \n",
    "            for i, output in enumerate(model_outputs):\n",
    "                user_prompt += f\"Response {i}: {output}\\n\"\n",
    "\n",
    "            user_prompt += end_prompt\n",
    "            \n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            sample = {\"idx\": idx, \"prompt\": message, \"entry\": entry}\n",
    "            samples.append(sample)\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "    return samples\n",
    "\n",
    "def generate_model_output(model: str, prompt: str, temperature: float = 1.0) -> str:        \n",
    "    responses = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=prompt,\n",
    "        n=1,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    outputs = [choice.message.content for choice in responses.choices]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    save_path = f\"{config.output_dir}/{config.task}/{config.model}/{config.task}_{config.shot_type}.jsonl\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    samples = construct_prompt(config)\n",
    "    if config.num_examples != -1: \n",
    "        samples = samples[:config.num_examples]\n",
    "\n",
    "    if samples:\n",
    "        print(f\"Model: {config.model} Task: {config.task}, Shot: {config.shot_type}\")\n",
    "        print(samples[0].keys())\n",
    "        print(\"-\" * 50)\n",
    "        prompt = samples[0][\"prompt\"]\n",
    "        for message in prompt:\n",
    "            print(f\"Role:\\n{message['role']}\")\n",
    "            print(f\"Content:\\n{message['content']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(f\"No samples found for Task: {config.task}, Shot: {config.shot_type}\")\n",
    "\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "            existing_data = {json.loads(line)['prompt'][1]['content'] for line in f}  \n",
    "\n",
    "    else:\n",
    "        existing_data = set()  \n",
    "\n",
    "\n",
    "    if samples:\n",
    "\n",
    "        with open(save_path, \"a\", encoding='utf-8') as f:  \n",
    "\n",
    "            for sample in tqdm(samples, total=len(samples)):\n",
    "                if sample['prompt'][1]['content'] in existing_data:  \n",
    "\n",
    "                    continue\n",
    "                try:\n",
    "\n",
    "                    model_outputs = generate_model_output(config.model, sample[\"prompt\"], config.temperature)\n",
    "                    sample[\"prompt_output\"] = model_outputs\n",
    "                    json.dump(sample, f)\n",
    "                    f.write(\"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample {sample['idx']}: {e}\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Results saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model = \"gpt-4o-mini\" \n",
    "        self.task = \"drop\"  # \"math500\", \"mmlu_pro\", \"gpqa\", \"drop\", \"hotpotqa\"\n",
    "        self.shot_type = \"few\"  \n",
    "        self.output_dir = \"usc\"\n",
    "        self.num_examples = -1\n",
    "        self.temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['math500', 'gpqa', \"drop\", \"hotpotqa\", \"musr_location\", 'musr_efficiently']\n",
    "# tasks = ['math500']\n",
    "subjects = ['business', 'law', 'psychology', 'biology', 'chemistry', 'history', 'other', 'health', 'economics', 'math', 'physics', 'computer science', 'philosophy', 'engineering']\n",
    "\n",
    "shots = [\"few\"]\n",
    "models = ['gpt-4o-mini']\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:00, 105474.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o-mini Task: math500, Shot: few\n",
      "dict_keys(['idx', 'prompt', 'entry'])\n",
      "--------------------------------------------------\n",
      "Role:\n",
      "system\n",
      "Content:\n",
      "\n",
      "--------------------------------------------------\n",
      "Role:\n",
      "user\n",
      "Content:\n",
      "I have generated the following responses to the question: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\n",
      "\n",
      "Response 0: To convert the point \\((0, 3)\\) from rectangular coordinates to polar coordinates, we apply the formulas for conversion:\n",
      "\n",
      "1. The radius \\(r\\) can be calculated using the formula:\n",
      "   \\[\n",
      "   r = \\sqrt{x^2 + y^2}\n",
      "   \\]\n",
      "   where \\(x = 0\\) and \\(y = 3\\).\n",
      "\n",
      "   Substituting the values, we get:\n",
      "   \\[\n",
      "   r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3.\n",
      "   \\]\n",
      "\n",
      "2. The angle \\(\\theta\\) is determined using the formula:\n",
      "   \\[\n",
      "   \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right).\n",
      "   \\]\n",
      "   However, since \\(x = 0\\), we cannot directly use this formula. When \\(x = 0\\), the angle \\(\\theta\\) corresponds to the positive \\(y\\)-axis.\n",
      "\n",
      "   In standard polar coordinate conventions:\n",
      "   - If \\(y > 0\\), \\(\\theta = \\frac{\\pi}{2}\\).\n",
      "   - If \\(y < 0\\), \\(\\theta = \\frac{3\\pi}{2}\\).\n",
      "\n",
      "   Since \\(y = 3\\) (which is positive), it follows that:\n",
      "   \\[\n",
      "   \\theta = \\frac{\\pi}{2}.\n",
      "   \\]\n",
      "\n",
      "Thus, the polar coordinates for the point \\((0, 3)\\) are:\n",
      "\n",
      "\\[\n",
      "(r, \\theta) = (3, \\frac{\\pi}{2}).\n",
      "\\]\n",
      "\n",
      "Therefore, the final answer is:\n",
      "\n",
      "\\(\\boxed{(3, \\frac{\\pi}{2})}\\)\n",
      "Response 1: To convert the point \\((0, 3)\\) from rectangular coordinates to polar coordinates, we use the formulas for polar coordinates:\n",
      "\n",
      "- The radial distance \\( r \\) is given by:\n",
      "  \\[\n",
      "  r = \\sqrt{x^2 + y^2}\n",
      "  \\]\n",
      "- The angle \\( \\theta \\) is given by:\n",
      "  \\[\n",
      "  \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)\n",
      "  \\]\n",
      "\n",
      "Step 1: Calculate \\( r \\):\n",
      "\\[\n",
      "x = 0, \\quad y = 3\n",
      "\\]\n",
      "\\[\n",
      "r = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3\n",
      "\\]\n",
      "\n",
      "Step 2: Calculate \\( \\theta \\):\n",
      "Since \\( x = 0 \\) and \\( y = 3 > 0 \\), the point lies on the positive \\( y \\)-axis. In this case, the angle \\( \\theta \\) corresponding to the positive \\( y \\)-axis is:\n",
      "\\[\n",
      "\\theta = \\frac{\\pi}{2}\n",
      "\\]\n",
      "\n",
      "Putting it all together, the polar coordinates are:\n",
      "\\[\n",
      "(r, \\theta) = (3, \\frac{\\pi}{2})\n",
      "\\]\n",
      "\n",
      "Thus, the final answer is \\(\\boxed{(3, \\frac{\\pi}{2})}\\).\n",
      "Response 2: To convert the point \\((0, 3)\\) in rectangular coordinates to polar coordinates, we need to find the values of \\(r\\) and \\(\\theta\\).\n",
      "\n",
      "1. **Calculate \\(r\\)**:  \n",
      "   The polar coordinate \\(r\\) is given by the distance from the point to the origin. This is calculated using the formula:\n",
      "   \\[\n",
      "   r = \\sqrt{x^2 + y^2}\n",
      "   \\]\n",
      "   Substituting \\(x = 0\\) and \\(y = 3\\):\n",
      "   \\[\n",
      "   r = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3\n",
      "   \\]\n",
      "\n",
      "2. **Calculate \\(\\theta\\)**:  \n",
      "   The angle \\(\\theta\\) is found using the arctangent function, which gives the angle corresponding to the coordinates:\n",
      "   \\[\n",
      "   \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)\n",
      "   \\]\n",
      "   Here \\(x = 0\\) and \\(y = 3\\). Since \\(x\\) is 0 and \\(y\\) is positive, the point lies on the positive \\(y\\)-axis, which corresponds to:\n",
      "   \\[\n",
      "   \\theta = \\frac{\\pi}{2}\n",
      "   \\]\n",
      "\n",
      "Thus, the polar coordinates are:\n",
      "\\[\n",
      "(r, \\theta) = (3, \\frac{\\pi}{2})\n",
      "\\]\n",
      "\n",
      "Putting this in the required format:\n",
      "\\[\n",
      "\\boxed{(3, \\frac{\\pi}{2})}\n",
      "\\]\n",
      "Response 3: To convert the point \\((0,3)\\) from rectangular coordinates to polar coordinates, we need to find the values of \\(r\\) and \\(\\theta\\).\n",
      "\n",
      "1. The formula for the radial coordinate \\(r\\) in polar coordinates is given by:\n",
      "   \\[\n",
      "   r = \\sqrt{x^2 + y^2}\n",
      "   \\]\n",
      "   For our point \\((0,3)\\):\n",
      "   \\[\n",
      "   r = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3\n",
      "   \\]\n",
      "\n",
      "2. The angular coordinate \\(\\theta\\) is determined by the arctangent of the ratio \\(y/x\\):\n",
      "   \\[\n",
      "   \\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)\n",
      "   \\]\n",
      "   For our point, since \\(x = 0\\) and \\(y = 3\\), we have:\n",
      "   \\[\n",
      "   \\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right)\n",
      "   \\]\n",
      "   This situation corresponds to \\(x = 0\\) and \\(y > 0\\), which occurs at \\(\\theta = \\frac{\\pi}{2}\\).\n",
      "\n",
      "Thus, the rectangular coordinates \\((0,3)\\) convert to the polar coordinates \\((r, \\theta)\\):\n",
      "\\[\n",
      "(r, \\theta) = (3, \\frac{\\pi}{2})\n",
      "\\]\n",
      "\n",
      "Putting this in the required format, we have the final answer:\n",
      "\n",
      "\\[\n",
      "\\boxed{(3, \\frac{\\pi}{2})}\n",
      "\\]\n",
      "Response 4: To convert the point \\((0, 3)\\) from rectangular coordinates to polar coordinates, we need to determine the values of \\(r\\) and \\(\\theta\\).\n",
      "\n",
      "1. **Calculate \\(r\\)**:\n",
      "   The polar coordinate \\(r\\) is the distance from the origin to the point \\((x, y)\\). It can be calculated using the formula:\n",
      "   \\[\n",
      "   r = \\sqrt{x^2 + y^2}\n",
      "   \\]\n",
      "   For our point \\((0, 3)\\):\n",
      "   \\[\n",
      "   r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3\n",
      "   \\]\n",
      "\n",
      "2. **Calculate \\(\\theta\\)**:\n",
      "   The angle \\(\\theta\\) is calculated from the positive x-axis. Since the point \\((0, 3)\\) lies directly above the origin on the y-axis, \\(\\theta\\) can be determined as:\n",
      "   \\[\n",
      "   \\theta = \\frac{\\pi}{2}\n",
      "   \\]\n",
      "   This is because the positive y-axis corresponds to the angle \\(\\frac{\\pi}{2}\\) radians in polar coordinates.\n",
      "\n",
      "3. **Combine the results**:\n",
      "   Therefore, the polar coordinates of the point \\((0, 3)\\) are:\n",
      "   \\[\n",
      "   (r, \\theta) = (3, \\frac{\\pi}{2})\n",
      "   \\]\n",
      "\n",
      "Thus, the final answer, in the form \\((r, \\theta)\\), is:\n",
      "\\[\n",
      "\\boxed{(3, \\frac{\\pi}{2})}\n",
      "\\]\n",
      "\n",
      "\n",
      "Evaluate these responses.\n",
      "Select the most consistent response based on majority consensus.\n",
      "Start your answer with \"The most consistent response is Response X\" (without quotes).\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:54<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to usc_reverse/math500/gpt-4o-mini/math500_few.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in models:\n",
    "    for task in tasks:\n",
    "        for shot in shots:\n",
    "            config.model = model\n",
    "            config.task = task\n",
    "            config.shot_type = shot\n",
    "            print(\"=\" * 50)\n",
    "            main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(response_text):\n",
    "    \"\"\"\n",
    "    주어진 응답 문자열에서 user 번호를 추출합니다.\n",
    "    \n",
    "    1. 먼저 '[user1]', 'user1' 등과 같이 user 뒤에 숫자가 붙은 패턴을 찾습니다.\n",
    "    2. 만약 해당 패턴 매치에 실패하면, 마지막 문장을 추출한 후 그 문장 내의 마지막 숫자를 반환합니다.\n",
    "    3. 모두 실패하면 None을 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    primary_pattern = r'\\[?\\s*The most consistent response is Response\\s*(\\d+)\\s*\\]?'\n",
    "    primary_matches = re.findall(primary_pattern, response_text, flags=re.IGNORECASE)\n",
    "    if primary_matches:\n",
    "\n",
    "        return int(primary_matches[0])\n",
    "    \n",
    "\n",
    "\n",
    "    sentences = re.split(r'[.\\n]', response_text.strip())\n",
    "\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if sentences:\n",
    "        last_sentence = sentences[-1]\n",
    "        fallback_matches = re.findall(r'(\\d+)', last_sentence)\n",
    "        if fallback_matches:\n",
    "\n",
    "            return int(fallback_matches[-1])\n",
    "    \n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gpt-4o-mini-math500: 100%|██████████| 500/500 [00:00<00:00, 209526.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing gpt-4o-mini-gpqa: 100%|██████████| 198/198 [00:00<00:00, 352342.89it/s]\n",
      "Processing gpt-4o-mini-drop: 100%|██████████| 500/500 [00:00<00:00, 417676.16it/s]\n",
      "Processing gpt-4o-mini-hotpotqa: 100%|██████████| 500/500 [00:00<00:00, 371967.36it/s]\n",
      "Processing gpt-4o-mini-musr_location: 100%|██████████| 256/256 [00:00<00:00, 340546.09it/s]\n",
      "Processing gpt-4o-mini-musr_efficiently: 100%|██████████| 250/250 [00:00<00:00, 256626.53it/s]\n"
     ]
    }
   ],
   "source": [
    "overall_summary = []\n",
    "\n",
    "for model in models:\n",
    "    for task in tasks:\n",
    "        if task == 'mmlu_pro':\n",
    "            continue\n",
    "        usc_path = f\"{config.output_dir}/{task}/{model}/{task}_{config.shot_type}.jsonl\"\n",
    "        usc = load_model_outputs(usc_path)\n",
    "\n",
    "        output_res_path = f\"../result/{task}/{model}/{task}_{config.shot_type}.jsonl\"\n",
    "        res = load_model_outputs(output_res_path)\n",
    "                    \n",
    "\n",
    "        user_counter = Counter()\n",
    "        total_entries = 0\n",
    "        for entry in tqdm(usc, desc=f\"Processing {model}-{task}\"):\n",
    "            total_entries += 1\n",
    "            raw_output = entry.get(\"prompt_output\")          \n",
    "            if isinstance(raw_output, list):\n",
    "                prompt_output = raw_output[0] if raw_output else None\n",
    "            else:\n",
    "                prompt_output = raw_output                   \n",
    "\n",
    "            if prompt_output is None:\n",
    "                user_counter[\"None\"] += 1\n",
    "                continue\n",
    "            extracted_user = extract_user_number(prompt_output)\n",
    "            if extracted_user is None:\n",
    "                user_counter[\"None\"] += 1\n",
    "            else:\n",
    "                user_counter[extracted_user] += 1\n",
    "\n",
    "\n",
    "        for user, count in user_counter.items():\n",
    "            percentage = (count / total_entries) * 100 if total_entries else 0\n",
    "            overall_summary.append({\n",
    "                \"Model\": model,\n",
    "                \"Task\": task,\n",
    "                \"Extracted User\": user,\n",
    "                \"Count\": count,\n",
    "                \"Percentage\": round(percentage, 2)\n",
    "            })\n",
    "\n",
    "        for i in range(len(res)):\n",
    "            raw_output = usc[i].get(\"prompt_output\")          \n",
    "            if isinstance(raw_output, list):\n",
    "                source_text = raw_output[0] if raw_output else None\n",
    "            else:\n",
    "                source_text = raw_output                   \n",
    "\n",
    "            if source_text is None:\n",
    "                answer_number = 0\n",
    "            else:\n",
    "                answer_number = extract_user_number(source_text)\n",
    "                if not isinstance(answer_number, int) or not (0 <= answer_number <= 4):\n",
    "                    answer_number = 0\n",
    "            model_outputs = res[i].get('model_outputs') or res[i].get('resps')[0]\n",
    "\n",
    "            model_outputs = model_outputs[::-1]\n",
    " \n",
    "            if len(model_outputs) <= answer_number:\n",
    "                print(f\"Warning: Entry {i} has less than {answer_number} model outputs. Using default output.\")\n",
    "                chosen_output = model_outputs[0] if model_outputs else None\n",
    "            else:\n",
    "                chosen_output = model_outputs[answer_number]\n",
    "            usc[i]['usc'] = chosen_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Task</th>\n",
       "      <th>drop</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>hotpotqa</th>\n",
       "      <th>math500</th>\n",
       "      <th>musr_efficiently</th>\n",
       "      <th>musr_location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90.4</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Task     drop   gpqa  hotpotqa  math500  musr_efficiently  musr_location\n",
       "Group                                                                   \n",
       "0-4     100.0  100.0     100.0    100.0              90.4          100.0\n",
       "Others    0.0    0.0       0.0      0.0               9.6            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_overall = pd.DataFrame(overall_summary)\n",
    "\n",
    "\n",
    "df_overall['ExtractedUser_Num'] = pd.to_numeric(\n",
    "    df_overall['Extracted User'], errors='coerce'\n",
    ")\n",
    "\n",
    "\n",
    "df_overall['Group'] = df_overall['ExtractedUser_Num'].apply(\n",
    "    lambda x: '0-4' if 0 <= x <= 4 else 'Others'\n",
    ")\n",
    "\n",
    "for model in df_overall[\"Model\"].unique():\n",
    "    model_df = df_overall[df_overall[\"Model\"] == model]\n",
    "    \n",
    "\n",
    "    pivot_table = model_df.pivot_table(\n",
    "        index=\"Group\",         \n",
    "        columns=\"Task\",         \n",
    "        values=\"Percentage\",    \n",
    "        aggfunc=\"sum\",          \n",
    "        fill_value=0            \n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel: {model}\")\n",
    "    display(pivot_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Task</th>\n",
       "      <th>drop</th>\n",
       "      <th>gpqa</th>\n",
       "      <th>hotpotqa</th>\n",
       "      <th>math500</th>\n",
       "      <th>musr_efficiently</th>\n",
       "      <th>musr_location</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extracted User</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.8</td>\n",
       "      <td>66.16</td>\n",
       "      <td>52.6</td>\n",
       "      <td>87.8</td>\n",
       "      <td>55.6</td>\n",
       "      <td>91.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.4</td>\n",
       "      <td>29.29</td>\n",
       "      <td>33.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>31.2</td>\n",
       "      <td>7.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Percentage                                           \\\n",
       "Task                 drop   gpqa hotpotqa math500 musr_efficiently   \n",
       "Extracted User                                                       \n",
       "-1                    0.0   0.00      0.0     0.0              9.6   \n",
       " 0                   81.8  66.16     52.6    87.8             55.6   \n",
       " 1                   15.4  29.29     33.6     9.4             31.2   \n",
       " 2                    1.2   0.51      7.4     1.0              2.4   \n",
       " 3                    0.6   0.00      3.8     0.4              1.2   \n",
       " 4                    1.0   4.04      2.6     1.4              0.0   \n",
       "\n",
       "                              \n",
       "Task           musr_location  \n",
       "Extracted User                \n",
       "-1                      0.00  \n",
       " 0                     91.41  \n",
       " 1                      7.42  \n",
       " 2                      0.39  \n",
       " 3                      0.39  \n",
       " 4                      0.39  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_overall = pd.DataFrame(overall_summary)\n",
    "\n",
    "for model in df_overall[\"Model\"].unique():\n",
    "    model_df = df_overall[df_overall[\"Model\"] == model]\n",
    "    \n",
    "    \n",
    "    pivot_table = model_df.pivot_table(\n",
    "        index=\"Extracted User\",       \n",
    "        columns=\"Task\",               \n",
    "        values=[\"Percentage\"], \n",
    "        fill_value=0           \n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel: {model}\")\n",
    "    display(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini-business: 100%|██████████| 300/300 [00:00<00:00, 357266.10it/s]\n",
      "gpt-4o-mini-law: 100%|██████████| 300/300 [00:00<00:00, 335008.31it/s]\n",
      "gpt-4o-mini-psychology: 100%|██████████| 300/300 [00:00<00:00, 356962.04it/s]\n",
      "gpt-4o-mini-biology: 100%|██████████| 300/300 [00:00<00:00, 362202.42it/s]\n",
      "gpt-4o-mini-chemistry: 100%|██████████| 300/300 [00:00<00:00, 348460.59it/s]\n",
      "gpt-4o-mini-history: 100%|██████████| 300/300 [00:00<00:00, 360335.40it/s]\n",
      "gpt-4o-mini-other: 100%|██████████| 300/300 [00:00<00:00, 327339.02it/s]\n",
      "gpt-4o-mini-health: 100%|██████████| 300/300 [00:00<00:00, 363562.90it/s]\n",
      "gpt-4o-mini-economics: 100%|██████████| 300/300 [00:00<00:00, 292421.85it/s]\n",
      "gpt-4o-mini-math: 100%|██████████| 300/300 [00:00<00:00, 355349.11it/s]\n",
      "gpt-4o-mini-physics: 100%|██████████| 300/300 [00:00<00:00, 351183.70it/s]\n",
      "gpt-4o-mini-computer science: 100%|██████████| 300/300 [00:00<00:00, 317029.78it/s]\n",
      "gpt-4o-mini-philosophy: 100%|██████████| 300/300 [00:00<00:00, 369976.83it/s]\n",
      "gpt-4o-mini-engineering: 100%|██████████| 300/300 [00:00<00:00, 377525.11it/s]\n",
      "gpt-4o-business: 100%|██████████| 300/300 [00:00<00:00, 133548.21it/s]\n",
      "gpt-4o-law: 100%|██████████| 300/300 [00:00<00:00, 235194.62it/s]\n",
      "gpt-4o-psychology: 100%|██████████| 300/300 [00:00<00:00, 324552.80it/s]\n",
      "gpt-4o-biology: 100%|██████████| 300/300 [00:00<00:00, 257740.93it/s]\n",
      "gpt-4o-chemistry: 100%|██████████| 300/300 [00:00<00:00, 94807.96it/s]\n",
      "gpt-4o-history: 100%|██████████| 300/300 [00:00<00:00, 345399.73it/s]\n",
      "gpt-4o-other: 100%|██████████| 300/300 [00:00<00:00, 234013.61it/s]\n",
      "gpt-4o-health: 100%|██████████| 300/300 [00:00<00:00, 288136.29it/s]\n",
      "gpt-4o-economics: 100%|██████████| 300/300 [00:00<00:00, 191404.20it/s]\n",
      "gpt-4o-math: 100%|██████████| 300/300 [00:00<00:00, 115460.75it/s]\n",
      "gpt-4o-physics: 100%|██████████| 300/300 [00:00<00:00, 99943.70it/s]\n",
      "gpt-4o-computer science: 100%|██████████| 300/300 [00:00<00:00, 132354.18it/s]\n",
      "gpt-4o-philosophy: 100%|██████████| 300/300 [00:00<00:00, 231261.02it/s]\n",
      "gpt-4o-engineering: 100%|██████████| 300/300 [00:00<00:00, 186856.43it/s]\n",
      "llama-business: 100%|██████████| 300/300 [00:00<00:00, 279744.60it/s]\n",
      "llama-law: 100%|██████████| 300/300 [00:00<00:00, 185945.20it/s]\n",
      "llama-psychology: 100%|██████████| 300/300 [00:00<00:00, 203672.90it/s]\n",
      "llama-biology: 100%|██████████| 300/300 [00:00<00:00, 190217.87it/s]\n",
      "llama-chemistry: 100%|██████████| 300/300 [00:00<00:00, 340538.89it/s]\n",
      "llama-history: 100%|██████████| 300/300 [00:00<00:00, 166749.43it/s]\n",
      "llama-other: 100%|██████████| 300/300 [00:00<00:00, 208050.79it/s]\n",
      "llama-health: 100%|██████████| 300/300 [00:00<00:00, 189216.72it/s]\n",
      "llama-economics: 100%|██████████| 300/300 [00:00<00:00, 198124.89it/s]\n",
      "llama-math: 100%|██████████| 300/300 [00:00<00:00, 347882.55it/s]\n",
      "llama-physics: 100%|██████████| 300/300 [00:00<00:00, 291541.06it/s]\n",
      "llama-computer science: 100%|██████████| 300/300 [00:00<00:00, 277952.55it/s]\n",
      "llama-philosophy: 100%|██████████| 300/300 [00:00<00:00, 185479.25it/s]\n",
      "llama-engineering: 100%|██████████| 300/300 [00:00<00:00, 300093.30it/s]\n"
     ]
    }
   ],
   "source": [
    "overall_summary_mmlu = []\n",
    "\n",
    "def process_mmlu_pro(model: str):\n",
    "    task = \"mmlu_pro\"\n",
    "\n",
    "    usc_path = (\n",
    "        f\"{config.output_dir}/{task}/{model}/\"\n",
    "        f\"{task}_{config.shot_type}.jsonl\"\n",
    "    )\n",
    "    all_entries = load_model_outputs(usc_path)\n",
    "\n",
    "    grouped = defaultdict(list)  \n",
    "    for entry in all_entries:\n",
    "        subj = entry[\"entry\"][\"category\"]         \n",
    "        grouped[subj].append(entry)\n",
    "\n",
    "    if model != 'llama': \n",
    "        for subj in grouped:\n",
    "            grouped[subj].sort(key=lambda e: e[\"idx\"])\n",
    "\n",
    "\n",
    "    for subject, entries in grouped.items():\n",
    "        output_res_path = f\"../result/{task}/{model}/{subject}_result.jsonl\"\n",
    "        res = load_model_outputs(output_res_path)\n",
    "\n",
    "\n",
    "        user_counter = Counter()\n",
    "        total = len(entries)\n",
    "\n",
    "        for idx, usc in enumerate(\n",
    "            tqdm(entries, desc=f\"{model}-{subject}\")\n",
    "        ):\n",
    "            \n",
    "            raw_output = usc.get(\"prompt_output\")          \n",
    "            if isinstance(raw_output, list):\n",
    "                p_out = raw_output[0] if raw_output else None\n",
    "            else:\n",
    "                p_out = raw_output                   \n",
    "\n",
    "            if p_out is None:\n",
    "                user_num = None\n",
    "            else:\n",
    "                user_num = extract_user_number(p_out)\n",
    "\n",
    "\n",
    "            key = user_num if user_num is not None else \"None\"\n",
    "            user_counter[key] += 1\n",
    "\n",
    "\n",
    "            if not isinstance(user_num, int) or not (0 <= user_num <= 4):\n",
    "                user_num = 0  # fallback\n",
    "\n",
    "\n",
    "            outs = res[idx].get(\"model_outputs\") or res[idx].get(\"resps\")[0]\n",
    "            chosen = (\n",
    "                outs[user_num - 1] if len(outs) >= user_num\n",
    "                else (outs[0] if outs else None)\n",
    "            )\n",
    "            entries[idx][\"usc\"] = chosen\n",
    "\n",
    "\n",
    "        dir_path = f\"{config.output_dir}/{task}/{model}/{subject}\"\n",
    "        file_name = f\"{task}_{config.shot_type}.jsonl\"\n",
    "        save_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "        for u, cnt in user_counter.items():\n",
    "            overall_summary_mmlu.append({\n",
    "                \"Model\": model,\n",
    "                \"Task\": task,\n",
    "                \"Subject\": subject,\n",
    "                \"Extracted User\": u,\n",
    "                \"Count\": cnt,\n",
    "                \"Percentage\": round(cnt / total * 100, 2)\n",
    "            })\n",
    "\n",
    "\n",
    "for model in ['gpt-4o-mini', 'gpt-4o', 'llama']:\n",
    "    process_mmlu_pro(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Task</th>\n",
       "      <th>mmlu_pro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extracted User</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.213571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.095714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.358462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Percentage\n",
       "Task             mmlu_pro\n",
       "Extracted User           \n",
       "-1               0.330000\n",
       " 0              73.214286\n",
       " 1              22.213571\n",
       " 2               1.095714\n",
       " 3               1.358462\n",
       " 4               2.142857"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Task</th>\n",
       "      <th>mmlu_pro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extracted User</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.832143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.762857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.595714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.110833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Percentage\n",
       "Task             mmlu_pro\n",
       "Extracted User           \n",
       "0               71.832143\n",
       "1               16.857143\n",
       "2                7.762857\n",
       "3                2.595714\n",
       "4                1.110833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: llama\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Task</th>\n",
       "      <th>mmlu_pro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extracted User</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.666429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.307857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.977857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.451429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Percentage\n",
       "Task             mmlu_pro\n",
       "Extracted User           \n",
       "-1               0.665000\n",
       " 0              34.666429\n",
       " 1              31.307857\n",
       " 2              11.500000\n",
       " 3              14.977857\n",
       " 4               7.451429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_mmlu = pd.DataFrame(overall_summary_mmlu)\n",
    "\n",
    "for model in df_mmlu[\"Model\"].unique():\n",
    "    model_df = df_mmlu[df_mmlu[\"Model\"] == model]\n",
    "    \n",
    "    pivot_table = model_df.pivot_table(\n",
    "        index=\"Extracted User\",       \n",
    "        columns=\"Task\",               \n",
    "        values=[\"Percentage\"], \n",
    "        fill_value=0           \n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel: {model}\")\n",
    "    display(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_eval = ['usc']\n",
    "# models = ['gpt-4o-mini', 'gpt-4o', 'llama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation Results for model=gpt-4o-mini, shot=few =====\n",
      "usc -> Accuracy: 0.7960\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from math500.math_utils import * \n",
    "from math500.parser import *\n",
    "from math500.grader import * \n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"{config.output_dir}/math500/{model}/math500_{config.shot_type}.jsonl\"\n",
    "    data = load_model_outputs(file_path)\n",
    "\n",
    "    scores = {k: [] for k in keys_to_eval}\n",
    "\n",
    "    for entry in data:\n",
    "        idx = entry[\"idx\"]\n",
    "        \n",
    "        _, gt = parse_ground_truth(entry['entry'], \"math\")\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        \n",
    "        for key in keys_to_eval:\n",
    "            if key not in entry:\n",
    "                continue\n",
    "\n",
    "\n",
    "            pred = extract_answer(entry[key], \"math\")\n",
    "            pred = strip_string(pred)\n",
    "\n",
    "\n",
    "            try:\n",
    "                result = math_equal_process((idx, pred, gt))\n",
    "\n",
    "                if not result :\n",
    "                    result = process_results(gt, [entry[key]])\n",
    "                    if not result:\n",
    "                        pred = extract_answer(pred, \"math\")\n",
    "                        result = math_equal_process((None, pred, gt))\n",
    "\n",
    "                scores[key].append(result)\n",
    "\n",
    "            except TimeoutError:\n",
    "                scores[key].append(False)\n",
    "            except Exception as error:\n",
    "                print(f\"Error while processing {key} for idx={idx}: {error}\")\n",
    "                scores[key].append(False)\n",
    "\n",
    "    print(f\"\\n===== Evaluation Results for model={model}, shot={config.shot_type} =====\")\n",
    "    for key in keys_to_eval:\n",
    "        if len(scores[key]) == 0:\n",
    "            print(f\"{key} -> No data / Not found in entries\")\n",
    "            continue\n",
    "\n",
    "        acc = sum(scores[key]) / len(scores[key])\n",
    "        print(f\"{key} -> Accuracy: {acc:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Results for model=gpt-4o-mini ===\n",
      "usc -> Accuracy: 0.6252\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Overall Results for model=gpt-4o ===\n",
      "usc -> Accuracy: 0.7210\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Overall Results for model=llama ===\n",
      "usc -> Accuracy: 0.3560\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_answer(text):\n",
    "    pattern = r\"answer is \\(?([A-J])\\)?\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return extract_again(text)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    overall_scores = {k: 0 for k in keys_to_eval}  \n",
    "    overall_total_entries = 0\n",
    "\n",
    "    for subject in subjects:\n",
    "        file_path = f\"{config.output_dir}/mmlu_pro/{model}/{subject}/mmlu_pro_few.jsonl\"\n",
    "        data = load_model_outputs(file_path)\n",
    "\n",
    "\n",
    "        subject_scores = {k: 0 for k in keys_to_eval}\n",
    "        total_data_len = len(data)\n",
    "        overall_total_entries += total_data_len\n",
    "\n",
    "        for entry in data:\n",
    "            model_outputs = entry.get('model_outputs', [])\n",
    "            answer = entry['entry'].get('answer') or entry['entry'].get('gold')\n",
    "\n",
    "\n",
    "            for key in keys_to_eval:\n",
    "                if key not in entry:\n",
    "                    continue\n",
    "                pred = extract_answer(entry[key])\n",
    "                if pred == answer: \n",
    "                    subject_scores[key] += 1\n",
    "\n",
    "\n",
    "        for k in keys_to_eval:\n",
    "            overall_scores[k] += subject_scores[k]\n",
    "\n",
    "           \n",
    "    print(f\"\\n=== Overall Results for model={model} ===\")\n",
    "    for key in keys_to_eval:\n",
    "        if overall_total_entries == 0:\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = overall_scores[key] / overall_total_entries\n",
    "        print(f\"{key} -> Accuracy: {acc:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('file is not a database')).History will not be written to the database.\n",
      "\n",
      "=== Results for model=gpt-4o-mini, shot=few ===\n",
      "usc -> Accuracy: 0.4242\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Results for model=gpt-4o, shot=few ===\n",
      "usc -> Accuracy: 0.5051\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Results for model=llama, shot=few ===\n",
      "usc -> Accuracy: 0.2879\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from gpqa.gpqa_utils import * \n",
    "\n",
    "examples = load_examples(\"../data/gpqa/gpqa_diamond.csv\", seed=0)\n",
    "\n",
    "for model in models:\n",
    "    file_path = f\"{config.output_dir}/gpqa/{model}/gpqa_{config.shot_type}.jsonl\"\n",
    "    data = load_model_outputs(file_path)\n",
    "\n",
    "    scores = {k: 0 for k in keys_to_eval}\n",
    "\n",
    "\n",
    "    total_data_len = len(data)\n",
    "    if total_data_len != len(examples):\n",
    "        print(\"Warning: data length and examples length do not match!\")\n",
    "\n",
    "    \n",
    "    \n",
    "    for entry, example in zip(data, examples):\n",
    "        correct_index = example.correct_index  \n",
    "\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        \n",
    "        for key in keys_to_eval:\n",
    "            if key not in entry:\n",
    "                continue\n",
    "\n",
    "            pred = parse_sampled_answer(entry[key])\n",
    "            \n",
    "            if pred is None:\n",
    "                is_correct = False\n",
    "            else:\n",
    "                is_correct = (LETTER_TO_INDEX[pred] == correct_index)\n",
    "\n",
    "            scores[key] += int(is_correct)\n",
    "    \n",
    "    print(f\"\\n=== Results for model={model}, shot={config.shot_type} ===\")\n",
    "    for key in keys_to_eval:\n",
    "        acc = scores[key] / total_data_len if total_data_len else 0\n",
    "        print(f\"{key} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "\n",
      "===== Results for model=gpt-4o-mini =====\n",
      "usc -> EM: 0.7880, F1: 0.8579\n",
      "--------------------------------------------------\n",
      "gpt-4o\n",
      "\n",
      "===== Results for model=gpt-4o =====\n",
      "usc -> EM: 0.8200, F1: 0.9020\n",
      "--------------------------------------------------\n",
      "llama\n",
      "\n",
      "===== Results for model=llama =====\n",
      "usc -> EM: 0.6960, F1: 0.7579\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from drop.drop_utils import *\n",
    "\n",
    "for model in models: \n",
    "    print(model)\n",
    "    file_path = f\"{config.output_dir}/drop/{model}/drop_{config.shot_type}.jsonl\"\n",
    "    data = load_model_outputs(file_path)\n",
    "    \n",
    "    em_scores = {k: [] for k in keys_to_eval}\n",
    "    f1_scores = {k: [] for k in keys_to_eval}\n",
    "\n",
    "    def get_max_em_f1(pred, golds):\n",
    "        max_em, max_f1 = 0.0, 0.0\n",
    "        for gold_answer in golds:\n",
    "            exact_match, f1_score = get_metrics(pred, gold_answer)\n",
    "            if gold_answer[0].strip():\n",
    "                max_em = max(max_em, exact_match)\n",
    "                max_f1 = max(max_f1, f1_score)\n",
    "        return max_em, max_f1\n",
    "\n",
    "    for entry in data:\n",
    "        golds = get_answers(entry['entry']) \n",
    "\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        \n",
    "        for k in keys_to_eval:\n",
    "            if k not in entry:\n",
    "                continue\n",
    "\n",
    "            pred = extract_answer(entry[k])\n",
    "            em_val, f1_val = get_max_em_f1(pred, golds)\n",
    "            em_scores[k].append(em_val)\n",
    "            f1_scores[k].append(f1_val)\n",
    "\n",
    "    print(f\"\\n===== Results for model={model} =====\")\n",
    "    for k in keys_to_eval:\n",
    "        if len(em_scores[k]) == 0:\n",
    "            print(f\"{k}: No entries found, skip.\")\n",
    "            continue\n",
    "\n",
    "        em_mean = np.mean(em_scores[k])\n",
    "        f1_mean = np.mean(f1_scores[k])\n",
    "        print(f\"{k} -> EM: {em_mean:.4f}, F1: {f1_mean:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "\n",
      "=== Results for model=gpt-4o-mini ===\n",
      "usc -> EM: 0.3660, F1: 0.4818\n",
      "--------------------------------------------------\n",
      "gpt-4o\n",
      "\n",
      "=== Results for model=gpt-4o ===\n",
      "usc -> EM: 0.4580, F1: 0.6044\n",
      "--------------------------------------------------\n",
      "llama\n",
      "\n",
      "=== Results for model=llama ===\n",
      "usc -> EM: 0.2440, F1: 0.3247\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from hotpotqa.hotpotqa_utils import *\n",
    "\n",
    "def extract_answer(response_text):\n",
    "    match = re.search(r\"Answer\\s+(.+)\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "\n",
    "        answer = re.sub(r\"[.\\n]+$\", \"\", answer).strip()\n",
    "        return answer\n",
    "\n",
    "\n",
    "    match = re.search(r\"(?<!\\w)Answer[:\\s]+(.+?)(?:[.\\n]|$)\", response_text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "\n",
    "        answer = re.sub(r\"[.\\n]+$\", \"\", answer).strip()\n",
    "        return answer\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "dataset = json.load(open(f'../data/hotpotqa/BM25/hotpotqa-bm25.json'))\n",
    "with open(\"../hotpotqa/react_prompt.json\", 'r') as f:\n",
    "    fewshot = json.load(f)\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    file_path = f\"{config.output_dir}/hotpotqa/{model}/hotpotqa_{config.shot_type}.jsonl\"\n",
    "    data = load_model_outputs(file_path)\n",
    "    preds = {k: [] for k in keys_to_eval}\n",
    "\n",
    "    for entry in data:\n",
    "\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        \n",
    "        for k in keys_to_eval:\n",
    "            if k in entry:\n",
    "\n",
    "                answer = extract_answer(entry[k])\n",
    "                preds[k].append(answer)\n",
    "\n",
    "    print(f\"\\n=== Results for model={model} ===\")\n",
    "    for k in keys_to_eval:\n",
    "        if len(preds[k]) == 0:\n",
    "            print(f\"{k}: No entries found, skip.\")\n",
    "            continue\n",
    "\n",
    "        em_scores, f1_scores = get_em_f1(dataset, preds[k])\n",
    "        em_mean = em_scores.mean()\n",
    "        f1_mean = f1_scores.mean()\n",
    "        print(f\"{k} -> EM: {em_mean:.4f}, F1: {f1_mean:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musr.musr import MuSRDataset\n",
    "\n",
    "ta_path = '../data/musr/team_allocation.json'\n",
    "ta = MuSRDataset(ta_path)\n",
    "\n",
    "op_path = '../data/musr/object_placements.json'\n",
    "op = MuSRDataset(op_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results for model=gpt-4o-mini ===\n",
      "usc -> Accuracy: 0.5977\n",
      "--------------------------------------------------\n",
      "gpt-4o\n",
      "\n",
      "=== Results for model=gpt-4o ===\n",
      "usc -> Accuracy: 0.7344\n",
      "--------------------------------------------------\n",
      "llama\n",
      "\n",
      "=== Results for model=llama ===\n",
      "usc -> Accuracy: 0.5234\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "    file_path = f\"{config.output_dir}/musr_location/{model}/musr_location_{config.shot_type}.jsonl\"\n",
    "    data = load_model_outputs(file_path)\n",
    "    \n",
    "    \n",
    "    preds = {k: [] for k in keys_to_eval}\n",
    "\n",
    "\n",
    "    for test_idx, entry in enumerate(data):\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        for k in keys_to_eval:\n",
    "            if k in entry:  \n",
    "                preds[k].append(entry[k])\n",
    "            else:\n",
    "                preds[k].append(None)\n",
    "\n",
    "    total_data_len = len(data)\n",
    "    \n",
    "    scores = {k: 0 for k in keys_to_eval}\n",
    "    \n",
    "    for i, entry in enumerate(data):\n",
    "        if 'entry' not in entry:\n",
    "            continue\n",
    "        \n",
    "        for k in keys_to_eval:\n",
    "            pred_answer = preds[k][i]\n",
    "            if pred_answer is not None:\n",
    "                metrics = op.evaluate_response([pred_answer], op[i])\n",
    "                if metrics and metrics[0]['correct']:\n",
    "                    scores[k] += 1\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Results for model={model} ===\")\n",
    "    for k in keys_to_eval:\n",
    "        if len(preds[k]) == 0:\n",
    "            print(f\"{k}: No entries found, skip.\")\n",
    "            continue\n",
    "\n",
    "        acc = scores[k] / total_data_len if total_data_len else 0\n",
    "        print(f\"{k} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n",
      "\n",
      "=== Results for model=gpt-4o-mini ===\n",
      "usc -> Accuracy: 0.7640\n",
      "--------------------------------------------------\n",
      "gpt-4o\n",
      "\n",
      "=== Results for model=gpt-4o ===\n",
      "usc -> Accuracy: 0.8760\n",
      "--------------------------------------------------\n",
      "llama\n",
      "\n",
      "=== Results for model=llama ===\n",
      "usc -> Accuracy: 0.6720\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "    file_path = f\"{config.output_dir}/musr_efficiently/{model}/musr_efficiently_{config.shot_type}.jsonl\"\n",
    "    data = load_model_outputs(file_path)\n",
    "    \n",
    "    \n",
    "    preds = {k: [] for k in keys_to_eval}\n",
    "\n",
    "    for test_idx, entry in enumerate(data):\n",
    "        model_outputs = entry.get('model_outputs', [])\n",
    "        for k in keys_to_eval:\n",
    "            if k in entry:  \n",
    "                preds[k].append(entry[k])\n",
    "            else:\n",
    "                preds[k].append(None)\n",
    "\n",
    "    total_data_len = len(data)\n",
    "    \n",
    "    scores = {k: 0 for k in keys_to_eval}\n",
    "    \n",
    "    for i, entry in enumerate(data):\n",
    "        if 'entry' not in entry:\n",
    "            continue\n",
    "        \n",
    "        for k in keys_to_eval:\n",
    "            pred_answer = preds[k][i]\n",
    "            if pred_answer is not None:\n",
    "                metrics = ta.evaluate_response([pred_answer], ta[i])\n",
    "                if metrics and metrics[0]['correct']:\n",
    "                    scores[k] += 1\n",
    "\n",
    "\n",
    "    print(f\"\\n=== Results for model={model} ===\")\n",
    "    for k in keys_to_eval:\n",
    "        if len(preds[k]) == 0:\n",
    "            print(f\"{k}: No entries found, skip.\")\n",
    "            continue\n",
    "\n",
    "        acc = scores[k] / total_data_len if total_data_len else 0\n",
    "        print(f\"{k} -> Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.15714285714286"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(78.6 + 62.5 + 42.4 + 78.8 + 36.6 + 59.8 + 76.4 ) / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.17142857142858"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(79.8 + 72.1 + 50.5 + 82.0 + 45.8 + 73.4 + 87.6 ) / 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.785714285714285"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(49.6 + 35.6 + 28.8 + 69.6 + 24.4 + + 52.3 + 67.2) /7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
