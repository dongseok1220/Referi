{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehdtjr1220/miniconda3/envs/proj2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "\n",
    "from gpqa.gpqa_utils import * \n",
    "\n",
    "from math500.math_utils import * \n",
    "from math500.parser import *\n",
    "from math500.grader import * \n",
    "\n",
    "from mmlu_pro.mmlu_utils import * \n",
    "\n",
    "from hotpotqa.hotpotqa_utils import *\n",
    "\n",
    "from drop.drop_utils import *\n",
    "\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['math500','gpqa', \"drop\", \"hotpotqa\", \"musr_location\", 'musr_efficiently']\n",
    "shots = [\"few\"]\n",
    "models = ['gpt-4o-mini', 'gpt-4o', 'llama']\n",
    "subjects = ['business', 'law', 'psychology', 'biology', 'chemistry', 'history', 'other', 'health', 'economics', 'math', 'physics', 'computer science', 'philosophy', 'engineering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(likelihood_file, baseline_few_file, baseline_few_zero_file): \n",
    "    if not os.path.exists(baseline_few_file):\n",
    "        print(f\"File not found: {baseline_few_file}\")\n",
    "        return None\n",
    "    else:\n",
    "        few_res = []\n",
    "        with open(baseline_few_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f, start=1):\n",
    "                try:\n",
    "                    few_res.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(baseline_few_file)\n",
    "                    print(f\"Error parsing line {i}: {e}\")\n",
    "                    print(line[:100])\n",
    "    \n",
    "    if not os.path.exists(baseline_few_zero_file):\n",
    "        print(f\"File not found: {baseline_few_zero_file}\")\n",
    "        return None\n",
    "    else:\n",
    "        few_zero_res = []\n",
    "        with open(baseline_few_zero_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f, start=1):\n",
    "                try:\n",
    "                    few_zero_res.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(baseline_few_zero_file)\n",
    "                    print(f\"Error parsing line {i}: {e}\")\n",
    "                    print(line[:100])\n",
    "\n",
    "    if not os.path.exists(likelihood_file):\n",
    "        print(f\"Error: {likelihood_file} not found.\")\n",
    "        return None\n",
    "\n",
    "    with open(likelihood_file, \"r\") as f:\n",
    "        likelihoods = json.load(f)\n",
    "    \n",
    "    problem_groups = list(zip(*likelihoods))\n",
    "    \n",
    "    num_examples = len(few_res)           \n",
    "\n",
    "    aggregated_results = {\n",
    "        \"forward\": [[] for _ in range(num_examples)],\n",
    "        \"direct\": [[] for _ in range(num_examples)],\n",
    "        \"no_replace\": [[] for _ in range(num_examples)],\n",
    "        \"backward\": [[] for _ in range(num_examples)],\n",
    "        \"is_correct\": [[] for _ in range(num_examples)]\n",
    "    }\n",
    "\n",
    "    for i, (few, few_zero, problem_likelihoods) in enumerate(zip(few_res, few_zero_res, problem_groups)):\n",
    "        problem_list = list(problem_likelihoods)\n",
    "\n",
    "        for cl in problem_list:\n",
    "            calculate_diffs(cl)\n",
    "\n",
    "        few_results = few['results']       \n",
    "        few_zero_results = few_zero['results']      \n",
    "        \n",
    "        \n",
    "        for j, (few_result, few_zero_result) in enumerate(zip(few_results, few_zero_results)):\n",
    "            \n",
    "            is_correct_val = problem_list[j]['is_correct']\n",
    "            \n",
    "            ce_list = few_result[\"ce_losses\"]   \n",
    "            ce_mean = np.mean(ce_list)\n",
    "\n",
    "            no_backward_val = problem_list[j]['no_replace_ce_diff']\n",
    "            backward_val = problem_list[j]['replace_ce_diff']\n",
    "\n",
    "            aggregated_results[\"forward\"][i].append(ce_mean)\n",
    "            aggregated_results[\"no_replace\"][i].append(no_backward_val)\n",
    "            aggregated_results[\"backward\"][i].append(backward_val)\n",
    "            aggregated_results[\"is_correct\"][i].append(is_correct_val)\n",
    "\n",
    "            ce_list = few_zero_result[\"ce_losses\"]   \n",
    "            ce_mean = np.mean(ce_list)\n",
    "\n",
    "            aggregated_results[\"direct\"][i].append(ce_mean)\n",
    "\n",
    "    for key in aggregated_results:\n",
    "        aggregated_results[key] = np.array(aggregated_results[key], dtype=np.float32)\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for task in tasks:\n",
    "    if task not in results:\n",
    "        results[task] = {}\n",
    "    for model in models:\n",
    "        likelihood_file = f\"likelihood/{task}/{model}/few/all_likelihoods.json\"\n",
    "        baseline_few_file = f\"baselines/baseline/{task}/{model}/{task}_few_few.jsonl\"\n",
    "        baseline_few_zero_file = f\"baselines/baseline/{task}/{model}/{task}_few_zero.jsonl\"\n",
    "        \n",
    "        result = get_value(likelihood_file, baseline_few_file, baseline_few_zero_file)\n",
    "        \n",
    "        results[task][model] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_results = {}\n",
    "\n",
    "for model in models:\n",
    "    mmlu_results[model] = {}  \n",
    "    for subject in subjects: \n",
    "        baseline_few_file = f\"baselines/baseline/mmlu_pro/{model}/{subject}/mmlu_pro_few_few.jsonl\"\n",
    "        baseline_few_zero_file = f\"baselines/baseline/mmlu_pro/{model}/{subject}/mmlu_pro_few_zero.jsonl\"\n",
    "        likelihoods_file = f\"likelihood/mmlu_pro/{model}/{subject}/all_likelihoods.json\"\n",
    "    \n",
    "        result = get_value(likelihoods_file, baseline_few_file, baseline_few_zero_file)\n",
    "    \n",
    "        mmlu_results[model][subject] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics    = ['direct']\n",
    "benchmarks = [\n",
    "    \"math500\",\n",
    "    \"mmlu_pro\",\n",
    "    \"gpqa\",\n",
    "    \"drop\",\n",
    "    \"hotpotqa\",\n",
    "    \"musr_location\",\n",
    "    \"musr_efficiently\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    cols = benchmarks + ['Avg']\n",
    "    df = pd.DataFrame(index=metrics, columns=cols, dtype=object)\n",
    "\n",
    "    metric_values = {m: [] for m in metrics}\n",
    "    if 'mmlu_pro' in benchmarks:\n",
    "        for subject, res in mmlu_results.get(model, {}).items():\n",
    "            if res is None:\n",
    "                print(f\"[WARNING] {model}/{subject} is None. Skipping.\")\n",
    "                continue\n",
    "            is_corr = res['is_correct']      # shape (N, C)\n",
    "            N       = is_corr.shape[0]\n",
    "            for m in metrics:\n",
    "                arr = res.get(m)\n",
    "                if arr is None:\n",
    "                    continue\n",
    "                chosen = arr.argmin(axis=-1)\n",
    "                acc    = 100 * is_corr[np.arange(N), chosen].mean()\n",
    "                metric_values[m].append(round(acc, 1))\n",
    "        avg_metrics = {\n",
    "            m: round(np.mean(vals), 1) if vals else np.nan\n",
    "            for m, vals in metric_values.items()\n",
    "        }\n",
    "        for m in metrics:\n",
    "            df.at[m, 'mmlu_pro'] = avg_metrics[m]\n",
    "\n",
    "    for bm in [b for b in benchmarks if b != 'mmlu_pro']:\n",
    "        res = results.get(bm, {}).get(model)\n",
    "        if res is None:\n",
    "            df[bm] = np.nan\n",
    "            continue\n",
    "\n",
    "        is_corr = res['is_correct']\n",
    "        N       = is_corr.shape[0]\n",
    "\n",
    "        for m in metrics:\n",
    "            arr = res.get(m)\n",
    "            if arr is None:\n",
    "                df.at[m, bm] = np.nan\n",
    "                continue\n",
    "\n",
    "            chosen = arr.argmin(axis=-1)\n",
    "\n",
    "            if bm in ['drop', 'hotpotqa'] and is_corr.ndim == 3:\n",
    "                em = 100 * is_corr[np.arange(N), chosen, 0].mean()\n",
    "                f1 = 100 * is_corr[np.arange(N), chosen, 1].mean()\n",
    "                df.at[m, bm] = f\"EM: {em:.1f}, F1: {f1:.1f}\"\n",
    "            else:\n",
    "                acc = 100 * is_corr[np.arange(N), chosen].mean()\n",
    "                df.at[m, bm] = round(acc, 1)\n",
    "\n",
    "    for m in metrics:\n",
    "        em_vals = []\n",
    "        for bm in benchmarks:\n",
    "            cell = df.at[m, bm]\n",
    "            if pd.isna(cell):\n",
    "                continue\n",
    "            if isinstance(cell, str) and cell.startswith(\"EM:\"):\n",
    "                em_val = float(cell.split()[1].rstrip(','))\n",
    "                em_vals.append(em_val)\n",
    "            else:\n",
    "                try:\n",
    "                    em_vals.append(float(cell))\n",
    "                except:\n",
    "                    pass\n",
    "        df.at[m, 'Avg'] = round(np.mean(em_vals), 1) if em_vals else np.nan\n",
    "\n",
    "    print(f\"===== Model: {model} =====\")\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temps(beta, check):\n",
    "    backward  = check['backward']\n",
    "    no_replace   = check['no_replace']\n",
    "    direct     = check['direct']\n",
    "    forward      = check['forward']\n",
    "\n",
    "\n",
    "    no_replace_referi = (1 - beta) * direct + beta * (forward - no_replace)\n",
    "\n",
    "    referi = (1 - beta) * direct + beta * (forward - backward)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'no_replace_referi': no_replace_referi,\n",
    "        'referi': referi,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_list    = [0.5, 0.75, 1.0]\n",
    "\n",
    "methods = ['referi', 'no_replace_referi']\n",
    "\n",
    "ordered_benchmarks = [\n",
    "    \"math500\", \n",
    "    \"mmlu_pro\",\n",
    "    \"gpqa\",\n",
    "    \"drop\", \n",
    "    \"hotpotqa\",\n",
    "    \"musr_location\", \n",
    "    \"musr_efficiently\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"===== MODEL: {model} =====\")\n",
    "\n",
    "    \n",
    "    ce_rows = [m + \"_BEST\" for m in methods]\n",
    "    df   = pd.DataFrame(index=ce_rows,\n",
    "                           columns=ordered_benchmarks + ['Avg'],\n",
    "                           dtype=object)\n",
    "\n",
    "    for bm in ordered_benchmarks:\n",
    "        if bm == 'mmlu_pro':\n",
    "            for m in methods:\n",
    "                best_avg, best_alpha = None, None\n",
    "                for a in beta_list:\n",
    "                    vals = []\n",
    "                    for subj, res in mmlu_results.get(model, {}).items():\n",
    "                        if res is None:\n",
    "                            continue\n",
    "                        is_corr = res['is_correct']\n",
    "                        temps   = compute_temps(a, res)\n",
    "                        arr     = temps.get(m)\n",
    "                        if arr is None:\n",
    "                            continue\n",
    "                        chosen = np.argmin(arr, axis=-1)\n",
    "                        acc    = 100 * is_corr[np.arange(len(chosen)), chosen].mean()\n",
    "                        vals.append(acc)\n",
    "                    if not vals:\n",
    "                        continue\n",
    "                    avg_acc = round(np.mean(vals), 1)\n",
    "                    if best_avg is None or avg_acc > best_avg:\n",
    "                        best_avg, best_alpha = avg_acc, a\n",
    "                if best_avg is not None:\n",
    "                    df.at[f\"{m}_BEST\", 'mmlu_pro'] = f\"{best_avg:.1f} ({best_alpha:.1f})\"\n",
    "                else:\n",
    "                    df.at[f\"{m}_BEST\", 'mmlu_pro'] = np.nan\n",
    "\n",
    "        else:\n",
    "            res = results.get(bm, {}).get(model)\n",
    "            if res is None:\n",
    "                df[bm] = np.nan\n",
    "                continue\n",
    "\n",
    "            is_corr = res['is_correct']\n",
    "            N       = is_corr.shape[0]\n",
    "            is_drop = bm in ['drop', 'hotpotqa']\n",
    "\n",
    "            for m in methods:\n",
    "                best_score, best_alpha = None, None\n",
    "\n",
    "                for a in beta_list:\n",
    "                    temps = compute_temps(a, res)\n",
    "                    arr   = temps[m]\n",
    "                    chosen = np.argmin(arr, axis=-1)\n",
    "\n",
    "                    if is_drop:\n",
    "                        score = 100 * is_corr[np.arange(N), chosen, 0].mean()\n",
    "                    else:\n",
    "                        score = 100 * is_corr[np.arange(N), chosen].mean()\n",
    "\n",
    "                    if best_score is None or score > best_score:\n",
    "                        best_score, best_alpha = score, a\n",
    "\n",
    "                if best_score is None:\n",
    "                    df.at[f\"{m}_BEST\", bm] = np.nan\n",
    "                else:\n",
    "                    if is_drop:\n",
    "                        temps_best = compute_temps(best_alpha, res)\n",
    "                        arr_best   = temps_best[m]\n",
    "                        chosen_best= np.argmin(arr_best, axis=-1)\n",
    "                        best_f1    = 100 * is_corr[np.arange(N), chosen_best, 1].mean()\n",
    "\n",
    "                        df.at[f\"{m}_BEST\", bm] = (\n",
    "                            f\"EM: {best_score:.1f}({best_alpha:.1f}), \"\n",
    "                            f\"F1: {best_f1:.1f}({best_alpha:.1f})\"\n",
    "                        )\n",
    "                    else:\n",
    "                        df.at[f\"{m}_BEST\", bm] = f\"{best_score:.1f} ({best_alpha:.1f})\"\n",
    "\n",
    "    def compute_avg(df):\n",
    "        for idx in df.index:\n",
    "            vals = []\n",
    "            for bm in ordered_benchmarks:\n",
    "                cell = df.at[idx, bm]\n",
    "                if pd.isna(cell): continue\n",
    "                if bm in ['drop', 'hotpotqa']:\n",
    "                    try:\n",
    "                        em = float(cell.split(\"EM:\")[1].split(\"(\")[0])\n",
    "                        vals.append(em)\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        vals.append(float(cell.split()[0]))\n",
    "                    except:\n",
    "                        pass\n",
    "            df.at[idx, 'Avg'] = round(np.mean(vals), 1) if vals else np.nan\n",
    "\n",
    "    compute_avg(df)\n",
    "\n",
    "    print(\">>> BEST only:\")\n",
    "    display(df)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
